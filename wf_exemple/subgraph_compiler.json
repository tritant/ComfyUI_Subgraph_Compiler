{"id":"ada026ba-657d-4492-ae99-cb8374f86cdc","revision":0,"last_node_id":400,"last_link_id":13,"nodes":[{"id":400,"type":"PreviewImage","pos":[740,-1420],"size":[290,290],"flags":{},"order":2,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":13}],"outputs":[],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"PreviewImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.2.2"}},"widgets_values":[]},{"id":396,"type":"SubgraphCompiler","pos":[1100,-2060],"size":[1170,1210],"flags":{},"order":3,"mode":0,"inputs":[{"localized_name":"generated_code","name":"generated_code","type":"STRING","widget":{"name":"generated_code"},"link":null},{"name":"(Subgraph Reference)","type":"*","link":12}],"outputs":[],"properties":{"Node name for S&R":"SubgraphCompiler","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.2.2"}},"widgets_values":["# Fichier généré par le Subgraph Compiler\nfrom __future__ import annotations\nfrom PIL import Image, ImageOps, ImageSequence\nfrom PIL.PngImagePlugin import PngInfo\nfrom abc import ABC, abstractmethod\nfrom comfy import utils\nfrom comfy.cli_args import args\nfrom comfy.comfy_types import IO, ComfyNodeABC, InputTypeDict\nfrom comfy.comfy_types import IO, ComfyNodeABC, InputTypeDict, FileLocator\nfrom comfy.k_diffusion import sa_solver\nfrom comfy.k_diffusion import sampling as k_diffusion_sampling\nfrom comfy_api.internal import register_versions, ComfyAPIWithVersion\nfrom comfy_api.latest import io, ComfyExtension\nfrom comfy_api.version_list import supported_versions\nfrom comfy_config import config_parser\nfrom enum import Enum\nfrom enum import StrEnum\nfrom typing import Literal, TypedDict, Optional\nfrom typing_extensions import NotRequired\nimport comfy.clip_vision\nimport comfy.controlnet\nimport comfy.diffusers_load\nimport comfy.model_management\nimport comfy.sample\nimport comfy.samplers\nimport comfy.sd\nimport comfy.utils\nimport comfy_extras.nodes_slg\nimport folder_paths\nimport hashlib\nimport importlib\nimport inspect\nimport json\nimport latent_preview\nimport logging\nimport math\nimport node_helpers\nimport nodes\nimport numpy as np\nimport os\nimport random\nimport safetensors.torch\nimport sys\nimport time\nimport torch\nimport traceback\n\n# --- Définitions des classes de nœuds internes et de leurs dépendances ---\nclass DualCLIPLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"clip_name1\": (folder_paths.get_filename_list(\"text_encoders\"), ),\n                              \"clip_name2\": (folder_paths.get_filename_list(\"text_encoders\"), ),\n                              \"type\": ([\"sdxl\", \"sd3\", \"flux\", \"hunyuan_video\", \"hidream\", \"hunyuan_image\"], ),\n                              },\n                \"optional\": {\n                              \"device\": ([\"default\", \"cpu\"], {\"advanced\": True}),\n                             }}\n    RETURN_TYPES = (\"CLIP\",)\n    FUNCTION = \"load_clip\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    DESCRIPTION = \"[Recipes]\\n\\nsdxl: clip-l, clip-g\\nsd3: clip-l, clip-g / clip-l, t5 / clip-g, t5\\nflux: clip-l, t5\\nhidream: at least one of t5 or llama, recommended t5 and llama\\nhunyuan_image: qwen2.5vl 7b and byt5 small\"\n\n    def load_clip(self, clip_name1, clip_name2, type, device=\"default\"):\n        clip_type = getattr(comfy.sd.CLIPType, type.upper(), comfy.sd.CLIPType.STABLE_DIFFUSION)\n\n        clip_path1 = folder_paths.get_full_path_or_raise(\"text_encoders\", clip_name1)\n        clip_path2 = folder_paths.get_full_path_or_raise(\"text_encoders\", clip_name2)\n\n        model_options = {}\n        if device == \"cpu\":\n            model_options[\"load_device\"] = model_options[\"offload_device\"] = torch.device(\"cpu\")\n\n        clip = comfy.sd.load_clip(ckpt_paths=[clip_path1, clip_path2], embedding_directory=folder_paths.get_folder_paths(\"embeddings\"), clip_type=clip_type, model_options=model_options)\n        return (clip,)\n\nclass KSamplerSelect:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"sampler_name\": (comfy.samplers.SAMPLER_NAMES, ),\n                      }\n               }\n    RETURN_TYPES = (\"SAMPLER\",)\n    CATEGORY = \"sampling/custom_sampling/samplers\"\n\n    FUNCTION = \"get_sampler\"\n\n    def get_sampler(self, sampler_name):\n        sampler = comfy.samplers.sampler_object(sampler_name)\n        return (sampler, )\n\nclass VAEDecode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"samples\": (\"LATENT\", {\"tooltip\": \"The latent to be decoded.\"}),\n                \"vae\": (\"VAE\", {\"tooltip\": \"The VAE model used for decoding the latent.\"})\n            }\n        }\n    RETURN_TYPES = (\"IMAGE\",)\n    OUTPUT_TOOLTIPS = (\"The decoded image.\",)\n    FUNCTION = \"decode\"\n\n    CATEGORY = \"latent\"\n    DESCRIPTION = \"Decodes latent images back into pixel space images.\"\n\n    def decode(self, vae, samples):\n        images = vae.decode(samples[\"samples\"])\n        if len(images.shape) == 5: #Combine batches\n            images = images.reshape(-1, images.shape[-3], images.shape[-2], images.shape[-1])\n        return (images, )\n\nclass VAELoader:\n    @staticmethod\n    def vae_list():\n        vaes = folder_paths.get_filename_list(\"vae\")\n        approx_vaes = folder_paths.get_filename_list(\"vae_approx\")\n        sdxl_taesd_enc = False\n        sdxl_taesd_dec = False\n        sd1_taesd_enc = False\n        sd1_taesd_dec = False\n        sd3_taesd_enc = False\n        sd3_taesd_dec = False\n        f1_taesd_enc = False\n        f1_taesd_dec = False\n\n        for v in approx_vaes:\n            if v.startswith(\"taesd_decoder.\"):\n                sd1_taesd_dec = True\n            elif v.startswith(\"taesd_encoder.\"):\n                sd1_taesd_enc = True\n            elif v.startswith(\"taesdxl_decoder.\"):\n                sdxl_taesd_dec = True\n            elif v.startswith(\"taesdxl_encoder.\"):\n                sdxl_taesd_enc = True\n            elif v.startswith(\"taesd3_decoder.\"):\n                sd3_taesd_dec = True\n            elif v.startswith(\"taesd3_encoder.\"):\n                sd3_taesd_enc = True\n            elif v.startswith(\"taef1_encoder.\"):\n                f1_taesd_dec = True\n            elif v.startswith(\"taef1_decoder.\"):\n                f1_taesd_enc = True\n        if sd1_taesd_dec and sd1_taesd_enc:\n            vaes.append(\"taesd\")\n        if sdxl_taesd_dec and sdxl_taesd_enc:\n            vaes.append(\"taesdxl\")\n        if sd3_taesd_dec and sd3_taesd_enc:\n            vaes.append(\"taesd3\")\n        if f1_taesd_dec and f1_taesd_enc:\n            vaes.append(\"taef1\")\n        vaes.append(\"pixel_space\")\n        return vaes\n\n    @staticmethod\n    def load_taesd(name):\n        sd = {}\n        approx_vaes = folder_paths.get_filename_list(\"vae_approx\")\n\n        encoder = next(filter(lambda a: a.startswith(\"{}_encoder.\".format(name)), approx_vaes))\n        decoder = next(filter(lambda a: a.startswith(\"{}_decoder.\".format(name)), approx_vaes))\n\n        enc = comfy.utils.load_torch_file(folder_paths.get_full_path_or_raise(\"vae_approx\", encoder))\n        for k in enc:\n            sd[\"taesd_encoder.{}\".format(k)] = enc[k]\n\n        dec = comfy.utils.load_torch_file(folder_paths.get_full_path_or_raise(\"vae_approx\", decoder))\n        for k in dec:\n            sd[\"taesd_decoder.{}\".format(k)] = dec[k]\n\n        if name == \"taesd\":\n            sd[\"vae_scale\"] = torch.tensor(0.18215)\n            sd[\"vae_shift\"] = torch.tensor(0.0)\n        elif name == \"taesdxl\":\n            sd[\"vae_scale\"] = torch.tensor(0.13025)\n            sd[\"vae_shift\"] = torch.tensor(0.0)\n        elif name == \"taesd3\":\n            sd[\"vae_scale\"] = torch.tensor(1.5305)\n            sd[\"vae_shift\"] = torch.tensor(0.0609)\n        elif name == \"taef1\":\n            sd[\"vae_scale\"] = torch.tensor(0.3611)\n            sd[\"vae_shift\"] = torch.tensor(0.1159)\n        return sd\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"vae_name\": (s.vae_list(), )}}\n    RETURN_TYPES = (\"VAE\",)\n    FUNCTION = \"load_vae\"\n\n    CATEGORY = \"loaders\"\n\n    #TODO: scale factor?\n    def load_vae(self, vae_name):\n        if vae_name == \"pixel_space\":\n            sd = {}\n            sd[\"pixel_space_vae\"] = torch.tensor(1.0)\n        elif vae_name in [\"taesd\", \"taesdxl\", \"taesd3\", \"taef1\"]:\n            sd = self.load_taesd(vae_name)\n        else:\n            vae_path = folder_paths.get_full_path_or_raise(\"vae\", vae_name)\n            sd = comfy.utils.load_torch_file(vae_path)\n        vae = comfy.sd.VAE(sd=sd)\n        vae.throw_exception_if_invalid()\n        return (vae,)\n\nclass BasicScheduler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                     \"scheduler\": (comfy.samplers.SCHEDULER_NAMES, ),\n                     \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000}),\n                     \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n                      }\n               }\n    RETURN_TYPES = (\"SIGMAS\",)\n    CATEGORY = \"sampling/custom_sampling/schedulers\"\n\n    FUNCTION = \"get_sigmas\"\n\n    def get_sigmas(self, model, scheduler, steps, denoise):\n        total_steps = steps\n        if denoise < 1.0:\n            if denoise <= 0.0:\n                return (torch.FloatTensor([]),)\n            total_steps = int(steps/denoise)\n\n        sigmas = comfy.samplers.calculate_sigmas(model.get_model_object(\"model_sampling\"), scheduler, total_steps).cpu()\n        sigmas = sigmas[-(steps + 1):]\n        return (sigmas, )\n\nclass FluxGuidance:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\n            \"conditioning\": (\"CONDITIONING\", ),\n            \"guidance\": (\"FLOAT\", {\"default\": 3.5, \"min\": 0.0, \"max\": 100.0, \"step\": 0.1}),\n            }}\n\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"append\"\n\n    CATEGORY = \"advanced/conditioning/flux\"\n\n    def append(self, conditioning, guidance):\n        c = node_helpers.conditioning_set_values(conditioning, {\"guidance\": guidance})\n        return (c, )\n\nclass SamplerCustomAdvanced:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"noise\": (\"NOISE\", ),\n                    \"guider\": (\"GUIDER\", ),\n                    \"sampler\": (\"SAMPLER\", ),\n                    \"sigmas\": (\"SIGMAS\", ),\n                    \"latent_image\": (\"LATENT\", ),\n                     }\n                }\n\n    RETURN_TYPES = (\"LATENT\",\"LATENT\")\n    RETURN_NAMES = (\"output\", \"denoised_output\")\n\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling/custom_sampling\"\n\n    def sample(self, noise, guider, sampler, sigmas, latent_image):\n        latent = latent_image\n        latent_image = latent[\"samples\"]\n        latent = latent.copy()\n        latent_image = comfy.sample.fix_empty_latent_channels(guider.model_patcher, latent_image)\n        latent[\"samples\"] = latent_image\n\n        noise_mask = None\n        if \"noise_mask\" in latent:\n            noise_mask = latent[\"noise_mask\"]\n\n        x0_output = {}\n        callback = latent_preview.prepare_callback(guider.model_patcher, sigmas.shape[-1] - 1, x0_output)\n\n        disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n        samples = guider.sample(noise.generate_noise(latent), latent_image, sampler, sigmas, denoise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=noise.seed)\n        samples = samples.to(comfy.model_management.intermediate_device())\n\n        out = latent.copy()\n        out[\"samples\"] = samples\n        if \"x0\" in x0_output:\n            out_denoised = latent.copy()\n            out_denoised[\"samples\"] = guider.model_patcher.model.process_latent_out(x0_output[\"x0\"].cpu())\n        else:\n            out_denoised = out\n        return (out, out_denoised)\n\nclass DisableNoise:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":{\n                     }\n                }\n\n    RETURN_TYPES = (\"NOISE\",)\n    FUNCTION = \"get_noise\"\n    CATEGORY = \"sampling/custom_sampling/noise\"\n\n    def get_noise(self):\n        return (Noise_EmptyNoise(),)\n\nclass Noise_EmptyNoise:\n    def __init__(self):\n        self.seed = 0\n\n    def generate_noise(self, input_latent):\n        latent_image = input_latent[\"samples\"]\n        return torch.zeros(latent_image.shape, dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n\nclass RandomNoise(DisableNoise):\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"noise_seed\": (\"INT\", {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 0xffffffffffffffff,\n                    \"control_after_generate\": True,\n                }),\n            }\n        }\n\n    def get_noise(self, noise_seed):\n        return (Noise_RandomNoise(noise_seed),)\n\nclass Noise_RandomNoise:\n    def __init__(self, seed):\n        self.seed = seed\n\n    def generate_noise(self, input_latent):\n        latent_image = input_latent[\"samples\"]\n        batch_inds = input_latent[\"batch_index\"] if \"batch_index\" in input_latent else None\n        return comfy.sample.prepare_noise(latent_image, self.seed, batch_inds)\n\nclass BasicGuider:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\":\n                    {\"model\": (\"MODEL\",),\n                    \"conditioning\": (\"CONDITIONING\", ),\n                     }\n                }\n\n    RETURN_TYPES = (\"GUIDER\",)\n\n    FUNCTION = \"get_guider\"\n    CATEGORY = \"sampling/custom_sampling/guiders\"\n\n    def get_guider(self, model, conditioning):\n        guider = Guider_Basic(model)\n        guider.set_conds(conditioning)\n        return (guider,)\n\nclass Guider_Basic(comfy.samplers.CFGGuider):\n    def set_conds(self, positive):\n        self.inner_set_conds({\"positive\": positive})\n\nclass UNETLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"unet_name\": (folder_paths.get_filename_list(\"diffusion_models\"), ),\n                              \"weight_dtype\": ([\"default\", \"fp8_e4m3fn\", \"fp8_e4m3fn_fast\", \"fp8_e5m2\"],)\n                             }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_unet\"\n\n    CATEGORY = \"advanced/loaders\"\n\n    def load_unet(self, unet_name, weight_dtype):\n        model_options = {}\n        if weight_dtype == \"fp8_e4m3fn\":\n            model_options[\"dtype\"] = torch.float8_e4m3fn\n        elif weight_dtype == \"fp8_e4m3fn_fast\":\n            model_options[\"dtype\"] = torch.float8_e4m3fn\n            model_options[\"fp8_optimizations\"] = True\n        elif weight_dtype == \"fp8_e5m2\":\n            model_options[\"dtype\"] = torch.float8_e5m2\n\n        unet_path = folder_paths.get_full_path_or_raise(\"diffusion_models\", unet_name)\n        model = comfy.sd.load_diffusion_model(unet_path, model_options=model_options)\n        return (model,)\n\nclass ComfyNodeABC(ABC):\n    \"\"\"Abstract base class for Comfy nodes.  Includes the names and expected types of attributes.\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/server_overview\n    \"\"\"\n\n    DESCRIPTION: str\n    \"\"\"Node description, shown as a tooltip when hovering over the node.\n\n    Usage::\n\n        # Explicitly define the description\n        DESCRIPTION = \"Example description here.\"\n\n        # Use the docstring of the node class.\n        DESCRIPTION = cleandoc(__doc__)\n    \"\"\"\n    CATEGORY: str\n    \"\"\"The category of the node, as per the \"Add Node\" menu.\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/server_overview#category\n    \"\"\"\n    EXPERIMENTAL: bool\n    \"\"\"Flags a node as experimental, informing users that it may change or not work as expected.\"\"\"\n    DEPRECATED: bool\n    \"\"\"Flags a node as deprecated, indicating to users that they should find alternatives to this node.\"\"\"\n    API_NODE: Optional[bool]\n    \"\"\"Flags a node as an API node. See: https://docs.comfy.org/tutorials/api-nodes/overview.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def INPUT_TYPES(s) -> InputTypeDict:\n        \"\"\"Defines node inputs.\n\n        * Must include the ``required`` key, which describes all inputs that must be connected for the node to execute.\n        * The ``optional`` key can be added to describe inputs which do not need to be connected.\n        * The ``hidden`` key offers some advanced functionality.  More info at: https://docs.comfy.org/custom-nodes/backend/more_on_inputs#hidden-inputs\n\n        Comfy Docs: https://docs.comfy.org/custom-nodes/backend/server_overview#input-types\n        \"\"\"\n        return {\"required\": {}}\n\n    OUTPUT_NODE: bool\n    \"\"\"Flags this node as an output node, causing any inputs it requires to be executed.\n\n    If a node is not connected to any output nodes, that node will not be executed.  Usage::\n\n        OUTPUT_NODE = True\n\n    From the docs:\n\n    By default, a node is not considered an output. Set ``OUTPUT_NODE = True`` to specify that it is.\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/server_overview#output-node\n    \"\"\"\n    INPUT_IS_LIST: bool\n    \"\"\"A flag indicating if this node implements the additional code necessary to deal with OUTPUT_IS_LIST nodes.\n\n    All inputs of ``type`` will become ``list[type]``, regardless of how many items are passed in.  This also affects ``check_lazy_status``.\n\n    From the docs:\n\n    A node can also override the default input behaviour and receive the whole list in a single call. This is done by setting a class attribute `INPUT_IS_LIST` to ``True``.\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/lists#list-processing\n    \"\"\"\n    OUTPUT_IS_LIST: tuple[bool, ...]\n    \"\"\"A tuple indicating which node outputs are lists, but will be connected to nodes that expect individual items.\n\n    Connected nodes that do not implement `INPUT_IS_LIST` will be executed once for every item in the list.\n\n    A ``tuple[bool]``, where the items match those in `RETURN_TYPES`::\n\n        RETURN_TYPES = (IO.INT, IO.INT, IO.STRING)\n        OUTPUT_IS_LIST = (True, True, False) # The string output will be handled normally\n\n    From the docs:\n\n    In order to tell Comfy that the list being returned should not be wrapped, but treated as a series of data for sequential processing,\n    the node should provide a class attribute `OUTPUT_IS_LIST`, which is a ``tuple[bool]``, of the same length as `RETURN_TYPES`,\n    specifying which outputs which should be so treated.\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/lists#list-processing\n    \"\"\"\n\n    RETURN_TYPES: tuple[IO, ...]\n    \"\"\"A tuple representing the outputs of this node.\n\n    Usage::\n\n        RETURN_TYPES = (IO.INT, \"INT\", \"CUSTOM_TYPE\")\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/server_overview#return-types\n    \"\"\"\n    RETURN_NAMES: tuple[str, ...]\n    \"\"\"The output slot names for each item in `RETURN_TYPES`, e.g. ``RETURN_NAMES = (\"count\", \"filter_string\")``\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/server_overview#return-names\n    \"\"\"\n    OUTPUT_TOOLTIPS: tuple[str, ...]\n    \"\"\"A tuple of strings to use as tooltips for node outputs, one for each item in `RETURN_TYPES`.\"\"\"\n    FUNCTION: str\n    \"\"\"The name of the function to execute as a literal string, e.g. `FUNCTION = \"execute\"`\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/server_overview#function\n    \"\"\"\n\nclass InputTypeDict(TypedDict):\n    \"\"\"Provides type hinting for node INPUT_TYPES.\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/more_on_inputs\n    \"\"\"\n\n    required: NotRequired[dict[str, tuple[IO, InputTypeOptions]]]\n    \"\"\"Describes all inputs that must be connected for the node to execute.\"\"\"\n    optional: NotRequired[dict[str, tuple[IO, InputTypeOptions]]]\n    \"\"\"Describes inputs which do not need to be connected.\"\"\"\n    hidden: NotRequired[HiddenInputTypeDict]\n    \"\"\"Offers advanced functionality and server-client communication.\n\n    Comfy Docs: https://docs.comfy.org/custom-nodes/backend/more_on_inputs#hidden-inputs\n    \"\"\"\n\nclass IO(StrEnum):\n    \"\"\"Node input/output data types.\n\n    Includes functionality for ``\"*\"`` (`ANY`) and ``\"MULTI,TYPES\"``.\n    \"\"\"\n\n    STRING = \"STRING\"\n    IMAGE = \"IMAGE\"\n    MASK = \"MASK\"\n    LATENT = \"LATENT\"\n    BOOLEAN = \"BOOLEAN\"\n    INT = \"INT\"\n    FLOAT = \"FLOAT\"\n    COMBO = \"COMBO\"\n    CONDITIONING = \"CONDITIONING\"\n    SAMPLER = \"SAMPLER\"\n    SIGMAS = \"SIGMAS\"\n    GUIDER = \"GUIDER\"\n    NOISE = \"NOISE\"\n    CLIP = \"CLIP\"\n    CONTROL_NET = \"CONTROL_NET\"\n    VAE = \"VAE\"\n    MODEL = \"MODEL\"\n    LORA_MODEL = \"LORA_MODEL\"\n    LOSS_MAP = \"LOSS_MAP\"\n    CLIP_VISION = \"CLIP_VISION\"\n    CLIP_VISION_OUTPUT = \"CLIP_VISION_OUTPUT\"\n    STYLE_MODEL = \"STYLE_MODEL\"\n    GLIGEN = \"GLIGEN\"\n    UPSCALE_MODEL = \"UPSCALE_MODEL\"\n    AUDIO = \"AUDIO\"\n    WEBCAM = \"WEBCAM\"\n    POINT = \"POINT\"\n    FACE_ANALYSIS = \"FACE_ANALYSIS\"\n    BBOX = \"BBOX\"\n    SEGS = \"SEGS\"\n    VIDEO = \"VIDEO\"\n\n    ANY = \"*\"\n    \"\"\"Always matches any type, but at a price.\n\n    Causes some functionality issues (e.g. reroutes, link types), and should be avoided whenever possible.\n    \"\"\"\n    NUMBER = \"FLOAT,INT\"\n    \"\"\"A float or an int - could be either\"\"\"\n    PRIMITIVE = \"STRING,FLOAT,INT,BOOLEAN\"\n    \"\"\"Could be any of: string, float, int, or bool\"\"\"\n\n    def __ne__(self, value: object) -> bool:\n        if self == \"*\" or value == \"*\":\n            return False\n        if not isinstance(value, str):\n            return True\n        a = frozenset(self.split(\",\"))\n        b = frozenset(value.split(\",\"))\n        return not (b.issubset(a) or a.issubset(b))\n\nclass CLIPTextEncode(ComfyNodeABC):\n    @classmethod\n    def INPUT_TYPES(s) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"text\": (IO.STRING, {\"multiline\": True, \"dynamicPrompts\": True, \"tooltip\": \"The text to be encoded.\"}),\n                \"clip\": (IO.CLIP, {\"tooltip\": \"The CLIP model used for encoding the text.\"})\n            }\n        }\n    RETURN_TYPES = (IO.CONDITIONING,)\n    OUTPUT_TOOLTIPS = (\"A conditioning containing the embedded text used to guide the diffusion model.\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning\"\n    DESCRIPTION = \"Encodes a text prompt using a CLIP model into an embedding that can be used to guide the diffusion model towards generating specific images.\"\n\n    def encode(self, clip, text):\n        if clip is None:\n            raise RuntimeError(\"ERROR: clip input is invalid: None\\n\\nIf the clip is from a checkpoint loader node your checkpoint does not contain a valid clip or text encoder model.\")\n        tokens = clip.tokenize(text)\n        return (clip.encode_from_tokens_scheduled(tokens), )\n\nclass EmptySD3LatentImage:\n    def __init__(self):\n        self.device = comfy.model_management.intermediate_device()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 1024, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                              \"height\": (\"INT\", {\"default\": 1024, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096})}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"latent/sd3\"\n\n    def generate(self, width, height, batch_size=1):\n        latent = torch.zeros([batch_size, 16, height // 8, width // 8], device=self.device)\n        return ({\"samples\":latent}, )\n\n# --- Définition du nouveau nœud compilé ---\nclass Flux_all_in_one:\n    @classmethod\n    def INPUT_TYPES(s):\n        return { 'required': {\n            \"unet_name\": (['CreArtV2.9-ori-Hyper-Flux-Dev-Fp8-Unet.safetensors', 'CreArtV3.2-ori-Hyper-Flux-Dev-Fp8-Unet.safetensors', 'CreArtV4.3-Hyper-Flux-Dev-Fp8-Unet.safetensors', 'CreArt_Ultimate.safetensors', 'CreArt_Ultimate0.safetensors', 'CreArt_Ultimate1.safetensors', 'CreArt_Ultimate2.safetensors', 'CreArt_Ultimate3.safetensors', 'CreArt_Ultimate4.safetensors', 'Wan2_1-I2V-14B-480P_fp8_e4m3fn.safetensors', 'Wan2_1-I2V-14B-720P_fp8_e4m3fn.safetensors', 'flux1-canny-dev.safetensors', 'flux1-depth-dev.safetensors', 'flux1-dev-kontext_fp8_scaled.safetensors', 'flux1-krea-dev_fp8_scaled.safetensors', 'fluxfillV1FP8Turbo_v10.safetensors', 'ori-dev.safetensors', 'svdq-int4-CreArt_Ultimate.safetensors', 'svdq-int4-CreArt_Ultimate1.safetensors', 'svdq-int4-CreArt_Ultimate2.safetensors', 'svdq-int4_r32-flux.1-dev.safetensors', 'svdq-int4_r32-flux.1-kontext-dev.safetensors', 'svdq-int4_r32-flux.1-krea-dev.safetensors', 'svdq-int4_r32-qwen-image-edit-2509-lightningv2.0-4steps.safetensors', 'svdq-int4_r32-qwen-image-edit-2509.safetensors', 'wan2.1_t2v_14B_fp8_e4m3fn.safetensors'],),\n            \"clip_name1\": (['awq-int4-flux.1-t5xxl.safetensors', 'clip_g.safetensors', 'clip_l.safetensors', 'llava_llama3_fp8_scaled.safetensors', 'open-clip-xlm-roberta-large-vit-huge-14_visual_fp16.safetensors', 'qwen_2.5_vl_7b_fp8_scaled.safetensors', 't5xxl_fp16.safetensors', 't5xxl_fp8_e4m3fn.safetensors', 'umt5-xxl-enc-fp8_e4m3fn.safetensors', 'umt5_xxl_fp8_e4m3fn_scaled.safetensors'],),\n            \"clip_name2\": (['awq-int4-flux.1-t5xxl.safetensors', 'clip_g.safetensors', 'clip_l.safetensors', 'llava_llama3_fp8_scaled.safetensors', 'open-clip-xlm-roberta-large-vit-huge-14_visual_fp16.safetensors', 'qwen_2.5_vl_7b_fp8_scaled.safetensors', 't5xxl_fp16.safetensors', 't5xxl_fp8_e4m3fn.safetensors', 'umt5-xxl-enc-fp8_e4m3fn.safetensors', 'umt5_xxl_fp8_e4m3fn_scaled.safetensors'],),\n            \"type\": (['sdxl', 'sd3', 'flux', 'hunyuan_video', 'hidream', 'hunyuan_image'],),\n            \"vae_name\": (['ae.sft', 'hunyuan_video_vae_bf16.safetensors', 'qwen_image_vae.safetensors', 'wan_2.1_vae.safetensors', 'taesd', 'taesdxl', 'taesd3', 'taef1', 'pixel_space'],),\n            \"width\": ('INT', {'default': 1024, 'min': 16, 'max': 16384, 'step': 16}),\n            \"height\": ('INT', {'default': 1024, 'min': 16, 'max': 16384, 'step': 16}),\n            \"guidance\": ('FLOAT', {'default': 3.5, 'min': 0.0, 'max': 100.0, 'step': 0.1}),\n            \"sampler_name\": (['euler', 'euler_cfg_pp', 'euler_ancestral', 'euler_ancestral_cfg_pp', 'heun', 'heunpp2', 'dpm_2', 'dpm_2_ancestral', 'lms', 'dpm_fast', 'dpm_adaptive', 'dpmpp_2s_ancestral', 'dpmpp_2s_ancestral_cfg_pp', 'dpmpp_sde', 'dpmpp_sde_gpu', 'dpmpp_2m', 'dpmpp_2m_cfg_pp', 'dpmpp_2m_sde', 'dpmpp_2m_sde_gpu', 'dpmpp_2m_sde_heun', 'dpmpp_2m_sde_heun_gpu', 'dpmpp_3m_sde', 'dpmpp_3m_sde_gpu', 'ddpm', 'lcm', 'ipndm', 'ipndm_v', 'deis', 'res_multistep', 'res_multistep_cfg_pp', 'res_multistep_ancestral', 'res_multistep_ancestral_cfg_pp', 'gradient_estimation', 'gradient_estimation_cfg_pp', 'er_sde', 'seeds_2', 'seeds_3', 'sa_solver', 'sa_solver_pece', 'ddim', 'uni_pc', 'uni_pc_bh2', 'supreme', 'dpmpp_3m_sde_dynamic_eta', 'euler_ancestral_dancing', 'lcm_custom_noise', 'ttm', 'clyb_4m_sde_momentumized', 'dpmpp_dualsde_momentumized', 'res_momentumized', 'bleh_preset_0'],),\n            \"scheduler\": (['simple', 'sgm_uniform', 'karras', 'exponential', 'ddim_uniform', 'beta', 'normal', 'linear_quadratic', 'kl_optimal'],),\n            \"steps\": ('INT', {'default': 20, 'min': 1, 'max': 10000}),\n            \"denoise\": ('FLOAT', {'default': 1.0, 'min': 0.0, 'max': 1.0, 'step': 0.01}),\n            \"noise_seed\": ('INT', {'default': 0, 'min': 0, 'max': 18446744073709551615, 'control_after_generate': True}),\n            \"text\": (IO.STRING, {'multiline': True, 'dynamicPrompts': True, 'tooltip': 'The text to be encoded.'}),\n            \"Load_Diffusion_Model_weight_dtype\": (['default', 'fp8_e4m3fn', 'fp8_e4m3fn_fast', 'fp8_e5m2'],),\n            \"EmptySD3LatentImage_batch_size\": ('INT', {'default': 1, 'min': 1, 'max': 4096}),\n        } }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = \"_my_nodes/custom\"\n\n    def execute(self, unet_name, clip_name1, clip_name2, type, vae_name, width, height, guidance, sampler_name, scheduler, steps, denoise, noise_seed, text, Load_Diffusion_Model_weight_dtype, EmptySD3LatentImage_batch_size):\n\n        # Exécution de Load Diffusion Model\n        Load_Diffusion_Model_389 = UNETLoader()\n        (out_389_0,) = Load_Diffusion_Model_389.load_unet(\n            unet_name=unet_name,\n            weight_dtype=Load_Diffusion_Model_weight_dtype,\n        )\n\n        # Exécution de DualCLIPLoader\n        DualCLIPLoader_390 = DualCLIPLoader()\n        (out_390_0,) = DualCLIPLoader_390.load_clip(\n            clip_name1=clip_name1,\n            clip_name2=clip_name2,\n            type=type,\n        )\n\n        # Exécution de Load VAE\n        Load_VAE_392 = VAELoader()\n        (out_392_0,) = Load_VAE_392.load_vae(\n            vae_name=vae_name,\n        )\n\n        # Exécution de EmptySD3LatentImage\n        EmptySD3LatentImage_393 = EmptySD3LatentImage()\n        (out_393_0,) = EmptySD3LatentImage_393.generate(\n            width=width,\n            height=height,\n            batch_size=EmptySD3LatentImage_batch_size,\n        )\n\n        # Exécution de KSamplerSelect\n        KSamplerSelect_397 = KSamplerSelect()\n        (out_397_0,) = KSamplerSelect_397.get_sampler(\n            sampler_name=sampler_name,\n        )\n\n        # Exécution de RandomNoise\n        RandomNoise_399 = RandomNoise()\n        (out_399_0,) = RandomNoise_399.get_noise(\n            noise_seed=noise_seed,\n        )\n\n        # Exécution de BasicScheduler\n        BasicScheduler_398 = BasicScheduler()\n        (out_398_0,) = BasicScheduler_398.get_sigmas(\n            model=out_389_0,\n            scheduler=scheduler,\n            steps=steps,\n            denoise=denoise,\n        )\n\n        # Exécution de CLIP Text Encode (Prompt)\n        CLIP_Text_Encode_Prompt_391 = CLIPTextEncode()\n        (out_391_0,) = CLIP_Text_Encode_Prompt_391.encode(\n            clip=out_390_0,\n            text=text,\n        )\n\n        # Exécution de FluxGuidance\n        FluxGuidance_395 = FluxGuidance()\n        (out_395_0,) = FluxGuidance_395.append(\n            conditioning=out_391_0,\n            guidance=guidance,\n        )\n\n        # Exécution de BasicGuider\n        BasicGuider_396 = BasicGuider()\n        (out_396_0,) = BasicGuider_396.get_guider(\n            model=out_389_0,\n            conditioning=out_395_0,\n        )\n\n        # Exécution de SamplerCustomAdvanced\n        SamplerCustomAdvanced_394 = SamplerCustomAdvanced()\n        (out_394_0, out_394_1,) = SamplerCustomAdvanced_394.sample(\n            latent_image=out_393_0,\n            guider=out_396_0,\n            sampler=out_397_0,\n            sigmas=out_398_0,\n            noise=out_399_0,\n        )\n\n        # Exécution de VAE Decode\n        VAE_Decode_400 = VAEDecode()\n        (out_400_0,) = VAE_Decode_400.decode(\n            vae=out_392_0,\n            samples=out_394_0,\n        )\n\n        return (out_400_0,)\n\n# --- Mappings pour ComfyUI ---\nNODE_CLASS_MAPPINGS = { \"Flux_all_in_one\": Flux_all_in_one }\nNODE_DISPLAY_NAME_MAPPINGS = { \"Flux_all_in_one\": \"Flux_all_in_one\" }\n","Flux_all_in_one","_my_nodes/custom","Copié dans le presse-papiers !","",null]},{"id":399,"type":"Flux_all_in_one","pos":[260,-1430],"size":[400,472],"flags":{},"order":0,"mode":4,"inputs":[{"localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"},"link":null},{"localized_name":"clip_name1","name":"clip_name1","type":"COMBO","widget":{"name":"clip_name1"},"link":null},{"localized_name":"clip_name2","name":"clip_name2","type":"COMBO","widget":{"name":"clip_name2"},"link":null},{"localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"},"link":null},{"localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"},"link":null},{"localized_name":"width","name":"width","type":"INT","widget":{"name":"width"},"link":null},{"localized_name":"height","name":"height","type":"INT","widget":{"name":"height"},"link":null},{"localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":null},{"localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":null},{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":null},{"localized_name":"Load_Diffusion_Model_weight_dtype","name":"Load_Diffusion_Model_weight_dtype","type":"COMBO","widget":{"name":"Load_Diffusion_Model_weight_dtype"},"link":null},{"localized_name":"EmptySD3LatentImage_batch_size","name":"EmptySD3LatentImage_batch_size","type":"INT","widget":{"name":"EmptySD3LatentImage_batch_size"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[13]}],"properties":{"Node name for S&R":"Flux_all_in_one","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.2.2"}},"widgets_values":["CreArtV4.3-Hyper-Flux-Dev-Fp8-Unet.safetensors","t5xxl_fp8_e4m3fn.safetensors","clip_l.safetensors","flux","ae.sft",1024,1024,3.5,"euler","simple",8,1,150668422662560,"randomize","","fp8_e4m3fn",1]},{"id":386,"type":"f3e568c1-907f-4620-b932-079c26ec2643","pos":[590,-2060],"size":[380,410],"flags":{},"order":1,"mode":0,"inputs":[{"name":"unet_name","type":"COMBO","widget":{"name":"unet_name"},"link":null},{"name":"clip_name1","type":"COMBO","widget":{"name":"clip_name1"},"link":null},{"name":"clip_name2","type":"COMBO","widget":{"name":"clip_name2"},"link":null},{"name":"type","type":"COMBO","widget":{"name":"type"},"link":null},{"name":"vae_name","type":"COMBO","widget":{"name":"vae_name"},"link":null},{"name":"width","type":"INT","widget":{"name":"width"},"link":null},{"name":"height","type":"INT","widget":{"name":"height"},"link":null},{"name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":null},{"name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":null},{"name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"name":"IMAGE","type":"IMAGE","links":[12]}],"title":"flux_all_in_one","properties":{"proxyWidgets":[],"cnr_id":"comfy-core","ver":"0.3.64","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.2.2"}},"widgets_values":["CreArtV2.9-ori-Hyper-Flux-Dev-Fp8-Unet.safetensors","awq-int4-flux.1-t5xxl.safetensors","awq-int4-flux.1-t5xxl.safetensors","flux","ae.sft",1024,1024,0,"euler","simple",20,1,1234,""]}],"links":[[12,386,0,396,1,"*"],[13,399,0,400,0,"IMAGE"]],"groups":[],"definitions":{"subgraphs":[{"id":"f3e568c1-907f-4620-b932-079c26ec2643","version":1,"state":{"lastGroupId":0,"lastNodeId":400,"lastLinkId":44,"lastRerouteId":0},"revision":0,"config":{},"name":"flux_all_in_one","inputNode":{"id":-10,"bounding":[1350,-1464,120,320]},"outputNode":{"id":-20,"bounding":[2160,-1464,120,60]},"inputs":[{"id":"e04ca0c0-0588-4e00-a037-b20b4e5be84a","name":"unet_name","type":"COMBO","linkIds":[17],"pos":[1450,-1444]},{"id":"b6ed535d-cde3-4398-a283-de7588cf1a52","name":"clip_name1","type":"COMBO","linkIds":[18],"pos":[1450,-1424]},{"id":"9fb5d91d-ad26-4538-9938-1467543fd38c","name":"clip_name2","type":"COMBO","linkIds":[19],"pos":[1450,-1404]},{"id":"8d8cccd7-32cf-414e-b694-27b57d6a805a","name":"type","type":"COMBO","linkIds":[20],"pos":[1450,-1384]},{"id":"5c5151cc-3bdb-469e-a790-f991396efb3f","name":"vae_name","type":"COMBO","linkIds":[22],"pos":[1450,-1364]},{"id":"de3c4bf3-23bd-40b0-b591-70c1aac7c1c2","name":"width","type":"INT","linkIds":[23],"pos":[1450,-1344]},{"id":"15cf7ed7-e735-46cc-9539-803ee8f585da","name":"height","type":"INT","linkIds":[24],"pos":[1450,-1324]},{"id":"ebbd611a-d833-4dd4-bcc0-2ac39c2a7993","name":"guidance","type":"FLOAT","linkIds":[27],"pos":[1450,-1304]},{"id":"19ca1c94-d52a-4e5f-8349-6ce64b6dc7bb","name":"sampler_name","type":"COMBO","linkIds":[32],"pos":[1450,-1284]},{"id":"af43968c-4f8b-4969-b6b2-5a9ba56cea30","name":"scheduler","type":"COMBO","linkIds":[35],"pos":[1450,-1264]},{"id":"fb67159b-297e-456b-95da-5cefb68e0988","name":"steps","type":"INT","linkIds":[36],"pos":[1450,-1244]},{"id":"3fdc889d-2706-4266-8531-0c420cbb6f5d","name":"denoise","type":"FLOAT","linkIds":[37],"pos":[1450,-1224]},{"id":"fa8fad8a-144c-4a0b-84ba-3623e645c51d","name":"noise_seed","type":"INT","linkIds":[39,40],"pos":[1450,-1204]},{"id":"038fd75f-66a2-43fd-b074-1694d488847a","name":"text","type":"STRING","linkIds":[44],"pos":[1450,-1184]}],"outputs":[{"id":"06293bda-0514-4076-87ba-8debcf361ee7","name":"IMAGE","type":"IMAGE","linkIds":[43],"pos":[2180,-1444]}],"widgets":[],"nodes":[{"id":390,"type":"DualCLIPLoader","pos":[1380,-1800],"size":[270,130],"flags":{},"order":1,"mode":0,"inputs":[{"localized_name":"clip_name1","name":"clip_name1","type":"COMBO","widget":{"name":"clip_name1"},"link":18},{"localized_name":"clip_name2","name":"clip_name2","type":"COMBO","widget":{"name":"clip_name2"},"link":19},{"localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"},"link":20},{"localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"},"link":null}],"outputs":[{"localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[21]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"DualCLIPLoader","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["awq-int4-flux.1-t5xxl.safetensors","awq-int4-flux.1-t5xxl.safetensors","flux","default"]},{"id":393,"type":"EmptySD3LatentImage","pos":[1070,-1940],"size":[270,106],"flags":{},"order":4,"mode":0,"inputs":[{"localized_name":"width","name":"width","type":"INT","widget":{"name":"width"},"link":23},{"localized_name":"height","name":"height","type":"INT","widget":{"name":"height"},"link":24},{"localized_name":"batch_size","name":"batch_size","type":"INT","widget":{"name":"batch_size"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[25]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"EmptySD3LatentImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[1024,1024,1]},{"id":394,"type":"SamplerCustomAdvanced","pos":[2150,-1690],"size":[203.1916717529297,106],"flags":{},"order":5,"mode":0,"inputs":[{"localized_name":"noise","name":"noise","type":"NOISE","link":38},{"localized_name":"guider","name":"guider","type":"GUIDER","link":29},{"localized_name":"sampler","name":"sampler","type":"SAMPLER","link":31},{"localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":34},{"localized_name":"latent_image","name":"latent_image","type":"LATENT","link":25}],"outputs":[{"localized_name":"output","name":"output","type":"LATENT","links":[42]},{"localized_name":"denoised_output","name":"denoised_output","type":"LATENT","links":null}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[]},{"id":396,"type":"BasicGuider","pos":[1810,-1600],"size":[156.02083206176758,46],"flags":{},"order":7,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":28},{"localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":30}],"outputs":[{"localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","links":[29]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"BasicGuider","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[]},{"id":395,"type":"FluxGuidance","pos":[2130,-1940],"size":[270,58],"flags":{},"order":6,"mode":0,"inputs":[{"localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":26},{"localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":27}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[30]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"FluxGuidance","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[0]},{"id":389,"type":"UNETLoader","pos":[1380,-1930],"size":[270,82],"flags":{},"order":0,"mode":0,"inputs":[{"localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"},"link":17},{"localized_name":"weight_dtype","name":"weight_dtype","type":"COMBO","widget":{"name":"weight_dtype"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[28,33]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"UNETLoader","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["CreArtV2.9-ori-Hyper-Flux-Dev-Fp8-Unet.safetensors","fp8_e4m3fn"]},{"id":398,"type":"BasicScheduler","pos":[1740,-1220],"size":[270,106],"flags":{},"order":9,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":33},{"localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":35},{"localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":36},{"localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":37}],"outputs":[{"localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[34]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"BasicScheduler","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["simple",20,1]},{"id":397,"type":"KSamplerSelect","pos":[1740,-1330],"size":[270,58],"flags":{},"order":8,"mode":0,"inputs":[{"localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":32}],"outputs":[{"localized_name":"SAMPLER","name":"SAMPLER","type":"SAMPLER","links":[31]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"KSamplerSelect","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["euler"]},{"id":399,"type":"RandomNoise","pos":[1740,-1490],"size":[270,82],"flags":{},"order":10,"mode":0,"inputs":[{"localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":40}],"outputs":[{"localized_name":"NOISE","name":"NOISE","type":"NOISE","links":[38]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"RandomNoise","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[1234,"randomize"]},{"id":392,"type":"VAELoader","pos":[1380,-1620],"size":[270,58],"flags":{},"order":3,"mode":0,"inputs":[{"localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"},"link":22}],"outputs":[{"localized_name":"VAE","name":"VAE","type":"VAE","links":[41]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"VAELoader","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["ae.sft"]},{"id":400,"type":"VAEDecode","pos":[2130,-1250],"size":[140,46],"flags":{},"order":11,"mode":0,"inputs":[{"localized_name":"samples","name":"samples","type":"LATENT","link":42},{"localized_name":"vae","name":"vae","type":"VAE","link":41}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[43]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"VAEDecode","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[]},{"id":391,"type":"CLIPTextEncode","pos":[1700,-1940],"size":[400,200],"flags":{},"order":2,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":21},{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":44}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[26]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"CLIPTextEncode","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[""]}],"groups":[],"links":[{"id":17,"origin_id":-10,"origin_slot":0,"target_id":389,"target_slot":0,"type":"COMBO"},{"id":18,"origin_id":-10,"origin_slot":1,"target_id":390,"target_slot":0,"type":"COMBO"},{"id":19,"origin_id":-10,"origin_slot":2,"target_id":390,"target_slot":1,"type":"COMBO"},{"id":20,"origin_id":-10,"origin_slot":3,"target_id":390,"target_slot":2,"type":"COMBO"},{"id":21,"origin_id":390,"origin_slot":0,"target_id":391,"target_slot":0,"type":"CLIP"},{"id":22,"origin_id":-10,"origin_slot":4,"target_id":392,"target_slot":0,"type":"COMBO"},{"id":23,"origin_id":-10,"origin_slot":5,"target_id":393,"target_slot":0,"type":"INT"},{"id":24,"origin_id":-10,"origin_slot":6,"target_id":393,"target_slot":1,"type":"INT"},{"id":25,"origin_id":393,"origin_slot":0,"target_id":394,"target_slot":4,"type":"LATENT"},{"id":26,"origin_id":391,"origin_slot":0,"target_id":395,"target_slot":0,"type":"CONDITIONING"},{"id":27,"origin_id":-10,"origin_slot":7,"target_id":395,"target_slot":1,"type":"FLOAT"},{"id":28,"origin_id":389,"origin_slot":0,"target_id":396,"target_slot":0,"type":"MODEL"},{"id":29,"origin_id":396,"origin_slot":0,"target_id":394,"target_slot":1,"type":"GUIDER"},{"id":30,"origin_id":395,"origin_slot":0,"target_id":396,"target_slot":1,"type":"CONDITIONING"},{"id":31,"origin_id":397,"origin_slot":0,"target_id":394,"target_slot":2,"type":"SAMPLER"},{"id":32,"origin_id":-10,"origin_slot":8,"target_id":397,"target_slot":0,"type":"COMBO"},{"id":33,"origin_id":389,"origin_slot":0,"target_id":398,"target_slot":0,"type":"MODEL"},{"id":34,"origin_id":398,"origin_slot":0,"target_id":394,"target_slot":3,"type":"SIGMAS"},{"id":35,"origin_id":-10,"origin_slot":9,"target_id":398,"target_slot":1,"type":"COMBO"},{"id":36,"origin_id":-10,"origin_slot":10,"target_id":398,"target_slot":2,"type":"INT"},{"id":37,"origin_id":-10,"origin_slot":11,"target_id":398,"target_slot":3,"type":"FLOAT"},{"id":38,"origin_id":399,"origin_slot":0,"target_id":394,"target_slot":0,"type":"NOISE"},{"id":40,"origin_id":-10,"origin_slot":12,"target_id":399,"target_slot":0,"type":"INT"},{"id":41,"origin_id":392,"origin_slot":0,"target_id":400,"target_slot":1,"type":"VAE"},{"id":42,"origin_id":394,"origin_slot":0,"target_id":400,"target_slot":0,"type":"LATENT"},{"id":43,"origin_id":400,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"IMAGE"},{"id":44,"origin_id":-10,"origin_slot":13,"target_id":391,"target_slot":1,"type":"STRING"}],"extra":{"ue_links":[],"links_added_by_ue":[]}}]},"config":{},"extra":{"ue_links":[],"links_added_by_ue":[],"ds":{"scale":0.5342857142857144,"offset":[651.5775401069516,2310]}},"version":0.4}