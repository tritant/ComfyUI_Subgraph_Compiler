{"id":"5097ce6e-0b7d-40dc-9ee7-3df8f5249a35","revision":0,"last_node_id":143,"last_link_id":311,"nodes":[{"id":117,"type":"PreviewImage","pos":[1150,-90],"size":[1060,1160],"flags":{},"order":4,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":307}],"outputs":[],"properties":{"cnr_id":"comfy-core","ver":"0.3.60","Node name for S&R":"PreviewImage","ue_properties":{"widget_ue_connectable":{},"version":"7.1","input_ue_unconnectable":{}}},"widgets_values":[]},{"id":134,"type":"LoadImage","pos":[-400,80],"size":[274.375,314],"flags":{},"order":0,"mode":0,"inputs":[{"localized_name":"image","name":"image","type":"COMBO","widget":{"name":"image"},"link":null},{"localized_name":"choose file to upload","name":"upload","type":"IMAGEUPLOAD","widget":{"name":"upload"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[298,299,306,308,309,310,311]},{"localized_name":"MASK","name":"MASK","type":"MASK","links":null}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"LoadImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.2.2"}},"widgets_values":["ComfyUI_temp_xyfum_00048_.png","image"]},{"id":143,"type":"Qwen_edit_all_in_one","pos":[-30,30],"size":[570,780],"flags":{},"order":2,"mode":0,"inputs":[{"localized_name":"image1","name":"image1","type":"IMAGE","link":298},{"localized_name":"image2","name":"image2","type":"IMAGE","link":308},{"localized_name":"image3","name":"image3","type":"IMAGE","link":310},{"localized_name":"image1_1","name":"image1_1","type":"IMAGE","link":306},{"localized_name":"image2_1","name":"image2_1","type":"IMAGE","link":309},{"localized_name":"image3_1","name":"image3_1","type":"IMAGE","link":311},{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":299},{"localized_name":"model_name","name":"model_name","type":"COMBO","widget":{"name":"model_name"},"link":null},{"localized_name":"clip_name","name":"clip_name","type":"COMBO","widget":{"name":"clip_name"},"link":null},{"localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"},"link":null},{"localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"},"link":null},{"localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"localized_name":"cfg","name":"cfg","type":"FLOAT","widget":{"name":"cfg"},"link":null},{"localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"localized_name":"prompt","name":"prompt","type":"STRING","widget":{"name":"prompt"},"link":null},{"localized_name":"prompt_1","name":"prompt_1","type":"STRING","widget":{"name":"prompt_1"},"link":null},{"localized_name":"cpu_offload","name":"cpu_offload","type":"COMBO","widget":{"name":"cpu_offload"},"link":null},{"localized_name":"seed","name":"seed","type":"INT","widget":{"name":"seed"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[307]}],"properties":{"Node name for S&R":"Qwen_edit_all_in_one","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.2.2"}},"widgets_values":["svdq-int4_r32-qwen-image-edit-2509-lightningv2.0-4steps.safetensors","qwen_2.5_vl_7b_fp8_scaled.safetensors","qwen_image","qwen_image_vae.safetensors","euler","beta",4,1,1,"a woman flying in the street","","auto",858125454112336,"randomize"]},{"id":126,"type":"7b4eca40-2d8f-4932-a22f-98d6507b84b6","pos":[620,-150],"size":[450,650],"flags":{},"order":1,"mode":4,"inputs":[{"name":"model_name","type":"COMBO","widget":{"name":"model_name"},"link":null},{"name":"clip_name","type":"COMBO","widget":{"name":"clip_name"},"link":null},{"name":"type","type":"COMBO","widget":{"name":"type"},"link":null},{"name":"vae_name","type":"COMBO","widget":{"name":"vae_name"},"link":null},{"name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"name":"cfg","type":"FLOAT","widget":{"name":"cfg"},"link":null},{"name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"name":"prompt","type":"STRING","widget":{"name":"prompt"},"link":null},{"name":"prompt_1","type":"STRING","widget":{"name":"prompt_1"},"link":null},{"name":"cpu_offload","type":"COMBO","widget":{"name":"cpu_offload"},"link":null},{"name":"seed","type":"INT","widget":{"name":"seed"},"link":null},{"name":"image1","type":"IMAGE","link":null},{"name":"image2","type":"IMAGE","link":null},{"name":"image3","type":"IMAGE","link":null},{"name":"image1_1","type":"IMAGE","link":null},{"name":"image2_1","type":"IMAGE","link":null},{"name":"image3_1","type":"IMAGE","link":null},{"name":"pixels","type":"IMAGE","link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[272]}],"properties":{"proxyWidgets":[["-1","model_name"],["-1","cpu_offload"],["-1","clip_name"],["-1","type"],["-1","vae_name"],["-1","sampler_name"],["-1","scheduler"],["-1","steps"],["-1","cfg"],["-1","denoise"],["-1","prompt"],["-1","prompt_1"],["-1","seed"]],"cnr_id":"comfy-core","ver":"0.3.64","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.2.2"}},"widgets_values":["svdq-int4_r32-qwen-image-edit-2509-lightningv2.0-4steps.safetensors","auto","svdq-int4_r32-qwen-image-edit-2509-lightningv2.0-4steps.safetensors","qwen_2.5_vl_7b_fp8_scaled.safetensors","qwen_image","qwen_image_vae.safetensors","euler","beta",4,1,"1","the woman in picture flying",""]},{"id":131,"type":"SubgraphCompiler","pos":[550,560],"size":[570,500],"flags":{},"order":3,"mode":0,"inputs":[{"localized_name":"generated_code","name":"generated_code","type":"STRING","widget":{"name":"generated_code"},"link":null},{"name":"(Subgraph Reference)","type":"*","link":272}],"outputs":[],"properties":{"Node name for S&R":"SubgraphCompiler","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.2.2"}},"widgets_values":["# ---Don't use this file for build_indexes--- \n\n# Fichier généré par le Subgraph Compiler (vFinal)\n\nfrom comfy import model_detection, model_management\nfrom comfy import model_management\nfrom comfy import utils\nfrom comfy.cli_args import args\nfrom comfy.comfy_types import IO, ComfyNodeABC, InputTypeDict, FileLocator\nfrom comfy.ldm.flux.layers import EmbedND\nfrom comfy.ldm.modules.attention import optimized_attention\nfrom comfy.ldm.modules.attention import optimized_attention_masked\nfrom comfy.ldm.qwen_image.model import GELU, FeedForward, LastLayer, QwenImageTransformer2DModel, QwenTimestepProjEmbeddings, apply_rotary_emb\nfrom comfy.model_base import ModelType, QwenImage\nfrom comfy.supported_models import QwenImage\nfrom comfy.utils import ProgressBar\nfrom comfy_api.latest import ComfyExtension, io\nfrom comfy_api.latest import io\nfrom comfy_api.latest import io, ComfyExtension\nfrom custom_mmpkg.custom_mmcv.utils import Registry, is_method_overridden\nfrom enum import Enum\nfrom enum import Enum, IntEnum, auto\nfrom functools import partial\nfrom math import ceil, pow, gcd\nfrom nodes import LoraLoader, ConditioningAverage, common_ksampler, ImageScale, ImageScaleBy, VAEEncode, VAEDecode\nfrom nunchaku.models.linear import AWQW4A16Linear, SVDQW4A4Linear\nfrom nunchaku.models.linear import SVDQW4A4Linear\nfrom nunchaku.models.utils import CPUOffloadManager\nfrom nunchaku.ops.fused import fused_gelu_mlp\nfrom nunchaku.utils import check_hardware_compatibility, get_gpu_memory, get_precision_from_quantization_config\nfrom timm.models.vision_transformer import Mlp, Attention as Attention_\nfrom torch import Tensor\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.init import trunc_normal_\nfrom torch.optim import Optimizer\nfrom torch.utils.checkpoint import checkpoint\nfrom torch.utils.checkpoint import checkpoint, checkpoint_sequential\nfrom typing import List\nfrom typing import List, Optional, Tuple, Union\nfrom typing import Optional, Tuple\nfrom typing import Optional, Tuple, Type\nfrom typing import Sequence, Tuple, Union, Callable, Optional, Dict, Any, List\nfrom warnings import warn\nimport comfy.clip_vision\nimport comfy.conds\nimport comfy.controlnet\nimport comfy.diffusers_load\nimport comfy.latent_formats\nimport comfy.model_base\nimport comfy.model_management\nimport comfy.model_patcher\nimport comfy.ops\nimport comfy.sample\nimport comfy.samplers\nimport comfy.sd\nimport comfy.supported_models_base\nimport comfy.utils\nimport folder_paths\nimport functools\nimport gc\nimport json\nimport logging\nimport math\nimport node_helpers\nimport time\nimport torch\nimport torch, copy\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nlogger = logging.getLogger(__name__)\n\n# --- Définitions des classes et fonctions nécessaires ---\nclass TextEncodeQwenImageEditPlus(io.ComfyNode):\n\n    @classmethod\n    def define_schema(cls):\n        return io.Schema(node_id='TextEncodeQwenImageEditPlus', category='advanced/conditioning', inputs=[io.Clip.Input('clip'), io.String.Input('prompt', multiline=True, dynamic_prompts=True), io.Vae.Input('vae', optional=True), io.Image.Input('image1', optional=True), io.Image.Input('image2', optional=True), io.Image.Input('image3', optional=True)], outputs=[io.Conditioning.Output()])\n\n    @classmethod\n    def execute(cls, clip, prompt, vae=None, image1=None, image2=None, image3=None) -> io.NodeOutput:\n        ref_latents = []\n        images = [image1, image2, image3]\n        images_vl = []\n        llama_template = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\"\n        image_prompt = ''\n        for i, image in enumerate(images):\n            if image is not None:\n                samples = image.movedim(-1, 1)\n                total = int(384 * 384)\n                scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))\n                width = round(samples.shape[3] * scale_by)\n                height = round(samples.shape[2] * scale_by)\n                s = comfy.utils.common_upscale(samples, width, height, 'area', 'disabled')\n                images_vl.append(s.movedim(1, -1))\n                if vae is not None:\n                    total = int(1024 * 1024)\n                    scale_by = math.sqrt(total / (samples.shape[3] * samples.shape[2]))\n                    width = round(samples.shape[3] * scale_by / 8.0) * 8\n                    height = round(samples.shape[2] * scale_by / 8.0) * 8\n                    s = comfy.utils.common_upscale(samples, width, height, 'area', 'disabled')\n                    ref_latents.append(vae.encode(s.movedim(1, -1)[:, :, :, :3]))\n                image_prompt += 'Picture {}: <|vision_start|><|image_pad|><|vision_end|>'.format(i + 1)\n        tokens = clip.tokenize(image_prompt + prompt, images=images_vl, llama_template=llama_template)\n        conditioning = clip.encode_from_tokens_scheduled(tokens)\n        if len(ref_latents) > 0:\n            conditioning = node_helpers.conditioning_set_values(conditioning, {'reference_latents': ref_latents}, append=True)\n        return io.NodeOutput(conditioning)\n\nclass CLIPLoader:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {'required': {'clip_name': (folder_paths.get_filename_list('text_encoders'),), 'type': (['stable_diffusion', 'stable_cascade', 'sd3', 'stable_audio', 'mochi', 'ltxv', 'pixart', 'cosmos', 'lumina2', 'wan', 'hidream', 'chroma', 'ace', 'omnigen2', 'qwen_image', 'hunyuan_image'],)}, 'optional': {'device': (['default', 'cpu'], {'advanced': True})}}\n    RETURN_TYPES = ('CLIP',)\n    FUNCTION = 'load_clip'\n    CATEGORY = 'advanced/loaders'\n    DESCRIPTION = '[Recipes]\\n\\nstable_diffusion: clip-l\\nstable_cascade: clip-g\\nsd3: t5 xxl/ clip-g / clip-l\\nstable_audio: t5 base\\nmochi: t5 xxl\\ncosmos: old t5 xxl\\nlumina2: gemma 2 2B\\nwan: umt5 xxl\\n hidream: llama-3.1 (Recommend) or t5\\nomnigen2: qwen vl 2.5 3B'\n\n    def load_clip(self, clip_name, type='stable_diffusion', device='default'):\n        clip_type = getattr(comfy.sd.CLIPType, type.upper(), comfy.sd.CLIPType.STABLE_DIFFUSION)\n        model_options = {}\n        if device == 'cpu':\n            model_options['load_device'] = model_options['offload_device'] = torch.device('cpu')\n        clip_path = folder_paths.get_full_path_or_raise('text_encoders', clip_name)\n        clip = comfy.sd.load_clip(ckpt_paths=[clip_path], embedding_directory=folder_paths.get_folder_paths('embeddings'), clip_type=clip_type, model_options=model_options)\n        return (clip,)\n\ndef model_name(self):\n    \"\"\"str: Name of the model, usually the module class name.\"\"\"\n    return self._model_name\n\nclass NunchakuModelPatcher(comfy.model_patcher.ModelPatcher):\n    \"\"\"\n    This class extends the ComfyUI ModelPatcher to provide custom logic for loading and unloading the model correctly.\n    \"\"\"\n\n    def load(self, device_to=None, lowvram_model_memory=0, force_patch_weights=False, full_load=False):\n        \"\"\"\n        Load the diffusion model onto the specified device.\n\n        Parameters\n        ----------\n        device_to : torch.device or str, optional\n            The device to which the diffusion model should be moved.\n        lowvram_model_memory : int, optional\n            Not used in this implementation.\n        force_patch_weights : bool, optional\n            Not used in this implementation.\n        full_load : bool, optional\n            Not used in this implementation.\n        \"\"\"\n        with self.use_ejected():\n            self.model.diffusion_model.to_safely(device_to)\n\n    def detach(self, unpatch_all: bool=True):\n        \"\"\"\n        Detach the model and move it to the offload device.\n\n        Parameters\n        ----------\n        unpatch_all : bool, optional\n            If True, unpatch all model components (default is True).\n        \"\"\"\n        self.eject_model()\n        self.model.diffusion_model.to_safely(self.offload_device)\n\ndef model_type(self, state_dict, prefix=''):\n    return comfy.model_base.ModelType.EPS\n\ndef offload(self):\n    if self.state == 1 and (self.offload_dtype != self.onload_dtype or self.offload_device != self.onload_device):\n        self.to(dtype=self.offload_dtype, device=self.offload_device)\n        self.state = 0\n\ndef mult(self, width, height, mult):\n    return (int(width * mult), int(height * mult))\n\ndef dim_out(self):\n    return self.embed_dims[-1]\n\nclass GELU(nn.Module):\n\n    def forward(self, x):\n        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n\nclass KSampler:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {'required': {'model': ('MODEL', {'tooltip': 'The model used for denoising the input latent.'}), 'seed': ('INT', {'default': 0, 'min': 0, 'max': 18446744073709551615, 'control_after_generate': True, 'tooltip': 'The random seed used for creating the noise.'}), 'steps': ('INT', {'default': 20, 'min': 1, 'max': 10000, 'tooltip': 'The number of steps used in the denoising process.'}), 'cfg': ('FLOAT', {'default': 8.0, 'min': 0.0, 'max': 100.0, 'step': 0.1, 'round': 0.01, 'tooltip': 'The Classifier-Free Guidance scale balances creativity and adherence to the prompt. Higher values result in images more closely matching the prompt however too high values will negatively impact quality.'}), 'sampler_name': (comfy.samplers.KSampler.SAMPLERS, {'tooltip': 'The algorithm used when sampling, this can affect the quality, speed, and style of the generated output.'}), 'scheduler': (comfy.samplers.KSampler.SCHEDULERS, {'tooltip': 'The scheduler controls how noise is gradually removed to form the image.'}), 'positive': ('CONDITIONING', {'tooltip': 'The conditioning describing the attributes you want to include in the image.'}), 'negative': ('CONDITIONING', {'tooltip': 'The conditioning describing the attributes you want to exclude from the image.'}), 'latent_image': ('LATENT', {'tooltip': 'The latent image to denoise.'}), 'denoise': ('FLOAT', {'default': 1.0, 'min': 0.0, 'max': 1.0, 'step': 0.01, 'tooltip': 'The amount of denoising applied, lower values will maintain the structure of the initial image allowing for image to image sampling.'})}}\n    RETURN_TYPES = ('LATENT',)\n    OUTPUT_TOOLTIPS = ('The denoised latent.',)\n    FUNCTION = 'sample'\n    CATEGORY = 'sampling'\n    DESCRIPTION = 'Uses the provided model, positive and negative conditioning to denoise the latent image.'\n\n    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):\n        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n\nclass VAEEncode:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {'required': {'pixels': ('IMAGE',), 'vae': ('VAE',)}}\n    RETURN_TYPES = ('LATENT',)\n    FUNCTION = 'encode'\n    CATEGORY = 'latent'\n\n    def encode(self, vae, pixels):\n        t = vae.encode(pixels[:, :, :, :3])\n        return ({'samples': t},)\n\nclass VAEDecode:\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {'required': {'samples': ('LATENT', {'tooltip': 'The latent to be decoded.'}), 'vae': ('VAE', {'tooltip': 'The VAE model used for decoding the latent.'})}}\n    RETURN_TYPES = ('IMAGE',)\n    OUTPUT_TOOLTIPS = ('The decoded image.',)\n    FUNCTION = 'decode'\n    CATEGORY = 'latent'\n    DESCRIPTION = 'Decodes latent images back into pixel space images.'\n\n    def decode(self, vae, samples):\n        images = vae.decode(samples['samples'])\n        if len(images.shape) == 5:\n            images = images.reshape(-1, images.shape[-3], images.shape[-2], images.shape[-1])\n        return (images,)\n\nclass NunchakuModelMixin:\n    \"\"\"\n    Mixin class for Nunchaku models.\n    \"\"\"\n    offload: bool = False\n\n    def set_offload(self, offload: bool, **kwargs):\n        \"\"\"\n        Enable or disable CPU offloading for the model.\n\n        Parameters\n        ----------\n        offload : bool\n            If True, enable CPU offloading. If False, disable it.\n        **kwargs\n            Additional keyword arguments for custom offload logic.\n\n        Raises\n        ------\n        NotImplementedError\n            If not implemented in the subclass.\n        \"\"\"\n        raise NotImplementedError('CPU offload needs to be implemented in the child class')\n\n    def to_safely(self, *args, **kwargs):\n        \"\"\"\n        Safely move the model to a device or change its dtype.\n\n        This method overrides the default ``.to()`` behavior to:\n\n        - Prevent moving the model to GPU if offload is enabled.\n        - Prevent changing the dtype of quantized models.\n\n        Parameters\n        ----------\n        *args\n            Positional arguments for device or dtype.\n        **kwargs\n            Keyword arguments for device or dtype.\n\n        Returns\n        -------\n        self : NunchakuModelMixin\n            The model itself, possibly moved to a new device or dtype.\n\n        Raises\n        ------\n        ValueError\n            If attempting to cast a quantized model to a new dtype after initialization.\n\n        Warns\n        -----\n        UserWarning\n            If attempting to move the model to GPU while offload is enabled.\n        \"\"\"\n        device_arg_or_kwarg_present = any((isinstance(arg, torch.device) for arg in args)) or 'device' in kwargs\n        dtype_present_in_args = 'dtype' in kwargs\n        for arg in args:\n            if not isinstance(arg, str):\n                continue\n            try:\n                torch.device(arg)\n                device_arg_or_kwarg_present = True\n            except RuntimeError:\n                pass\n        if not dtype_present_in_args:\n            for arg in args:\n                if isinstance(arg, torch.dtype):\n                    dtype_present_in_args = True\n                    break\n        if dtype_present_in_args:\n            raise ValueError('Casting a quantized model to a new `dtype` is unsupported. To set the dtype of unquantized layers, please use the `torch_dtype` argument when loading the model using `from_pretrained` or `from_single_file`')\n        if self.offload:\n            if device_arg_or_kwarg_present:\n                warn('Skipping moving the model to GPU as offload is enabled', UserWarning)\n                return self\n        return self.to(*args, **kwargs)\n\nclass NunchakuGELU(GELU):\n    \"\"\"\n    GELU activation with a quantized linear projection.\n\n    Parameters\n    ----------\n    dim_in : int\n        Input feature dimension.\n    dim_out : int\n        Output feature dimension.\n    approximate : str, optional\n        Approximation mode for GELU (default: \"none\").\n    bias : bool, optional\n        Whether to use bias in the projection (default: True).\n    dtype : torch.dtype, optional\n        Data type for the projection.\n    device : torch.device, optional\n        Device for the projection.\n    **kwargs\n        Additional arguments for the quantized linear layer.\n    \"\"\"\n\n    def __init__(self, dim_in: int, dim_out: int, approximate: str='none', bias: bool=True, dtype=None, device=None, **kwargs):\n        super(GELU, self).__init__()\n        self.proj = SVDQW4A4Linear(dim_in, dim_out, bias=bias, torch_dtype=dtype, device=device, **kwargs)\n        self.approximate = approximate\n\nclass Attention_custom_nodes_ComfyUI_nunchaku_models(nn.Module):\n    \"\"\"\n    Double-stream attention module for joint image-text attention.\n\n    This module fuses QKV projections for both image and text streams for improved speed,\n    applies Q/K normalization and rotary embeddings, and computes joint attention.\n\n    Parameters\n    ----------\n    query_dim : int\n        Input feature dimension.\n    dim_head : int, optional\n        Dimension per attention head (default: 64).\n    heads : int, optional\n        Number of attention heads (default: 8).\n    dropout : float, optional\n        Dropout probability (default: 0.0).\n    bias : bool, optional\n        Whether to use bias in projections (default: False).\n    eps : float, optional\n        Epsilon for normalization layers (default: 1e-5).\n    out_bias : bool, optional\n        Whether to use bias in output projections (default: True).\n    out_dim : int, optional\n        Output dimension for image stream.\n    out_context_dim : int, optional\n        Output dimension for text stream.\n    dtype : torch.dtype, optional\n        Data type for projections.\n    device : torch.device, optional\n        Device for projections.\n    operations : module, optional\n        Module providing normalization and linear layers.\n    **kwargs\n        Additional arguments for quantized linear layers.\n    \"\"\"\n\n    def __init__(self, query_dim: int, dim_head: int=64, heads: int=8, dropout: float=0.0, bias: bool=False, eps: float=1e-05, out_bias: bool=True, out_dim: int=None, out_context_dim: int=None, dtype=None, device=None, operations=None, **kwargs):\n        super().__init__()\n        self.inner_dim = out_dim if out_dim is not None else dim_head * heads\n        self.inner_kv_dim = self.inner_dim\n        self.heads = heads\n        self.dim_head = dim_head\n        self.out_dim = out_dim if out_dim is not None else query_dim\n        self.out_context_dim = out_context_dim if out_context_dim is not None else query_dim\n        self.dropout = dropout\n        self.norm_q = operations.RMSNorm(dim_head, eps=eps, elementwise_affine=True, dtype=dtype, device=device)\n        self.norm_k = operations.RMSNorm(dim_head, eps=eps, elementwise_affine=True, dtype=dtype, device=device)\n        self.norm_added_q = operations.RMSNorm(dim_head, eps=eps, dtype=dtype, device=device)\n        self.norm_added_k = operations.RMSNorm(dim_head, eps=eps, dtype=dtype, device=device)\n        self.to_qkv = SVDQW4A4Linear(query_dim, self.inner_dim + self.inner_kv_dim * 2, bias=bias, torch_dtype=dtype, device=device, **kwargs)\n        self.add_qkv_proj = SVDQW4A4Linear(query_dim, self.inner_dim + self.inner_kv_dim * 2, bias=bias, torch_dtype=dtype, device=device, **kwargs)\n        self.to_out = nn.ModuleList([SVDQW4A4Linear(self.inner_dim, self.out_dim, bias=out_bias, torch_dtype=dtype, device=device, **kwargs), nn.Dropout(dropout)])\n        self.to_add_out = SVDQW4A4Linear(self.inner_dim, self.out_context_dim, bias=out_bias, torch_dtype=dtype, device=device, **kwargs)\n\n    def forward(self, hidden_states: torch.FloatTensor, encoder_hidden_states: torch.FloatTensor=None, encoder_hidden_states_mask: torch.FloatTensor=None, attention_mask: Optional[torch.FloatTensor]=None, image_rotary_emb: Optional[torch.Tensor]=None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass for double-stream attention.\n\n        Parameters\n        ----------\n        hidden_states : torch.FloatTensor\n            Image stream input tensor of shape (batch, seq_len_img, dim).\n        encoder_hidden_states : torch.FloatTensor, optional\n            Text stream input tensor of shape (batch, seq_len_txt, dim).\n        encoder_hidden_states_mask : torch.FloatTensor, optional\n            Mask for encoder hidden states.\n        attention_mask : torch.FloatTensor, optional\n            Attention mask for joint attention.\n        image_rotary_emb : torch.Tensor, optional\n            Rotary positional embeddings.\n\n        Returns\n        -------\n        img_attn_output : torch.Tensor\n            Output tensor for image stream.\n        txt_attn_output : torch.Tensor\n            Output tensor for text stream.\n        \"\"\"\n        seq_txt = encoder_hidden_states.shape[1]\n        img_qkv = self.to_qkv(hidden_states)\n        img_query, img_key, img_value = img_qkv.chunk(3, dim=-1)\n        txt_qkv = self.add_qkv_proj(encoder_hidden_states)\n        txt_query, txt_key, txt_value = txt_qkv.chunk(3, dim=-1)\n        img_query = img_query.unflatten(-1, (self.heads, -1))\n        img_key = img_key.unflatten(-1, (self.heads, -1))\n        img_value = img_value.unflatten(-1, (self.heads, -1))\n        txt_query = txt_query.unflatten(-1, (self.heads, -1))\n        txt_key = txt_key.unflatten(-1, (self.heads, -1))\n        txt_value = txt_value.unflatten(-1, (self.heads, -1))\n        img_query = self.norm_q(img_query)\n        img_key = self.norm_k(img_key)\n        txt_query = self.norm_added_q(txt_query)\n        txt_key = self.norm_added_k(txt_key)\n        joint_query = torch.cat([txt_query, img_query], dim=1)\n        joint_key = torch.cat([txt_key, img_key], dim=1)\n        joint_value = torch.cat([txt_value, img_value], dim=1)\n        joint_query = apply_rotary_emb(joint_query, image_rotary_emb)\n        joint_key = apply_rotary_emb(joint_key, image_rotary_emb)\n        joint_query = joint_query.flatten(start_dim=2)\n        joint_key = joint_key.flatten(start_dim=2)\n        joint_value = joint_value.flatten(start_dim=2)\n        joint_hidden_states = optimized_attention_masked(joint_query, joint_key, joint_value, self.heads, attention_mask)\n        txt_attn_output = joint_hidden_states[:, :seq_txt, :]\n        img_attn_output = joint_hidden_states[:, seq_txt:, :]\n        img_attn_output = self.to_out[0](img_attn_output)\n        img_attn_output = self.to_out[1](img_attn_output)\n        txt_attn_output = self.to_add_out(txt_attn_output)\n        return (img_attn_output, txt_attn_output)\n\ndef dec(f):\n    cached_ret = None\n    cache_time = 0\n\n    def cached_func():\n        nonlocal cache_time, cached_ret\n        if time.time() > cache_time + duration or cached_ret is None:\n            cache_time = time.time()\n            cached_ret = f()\n        return cached_ret\n    return cached_func\n\nclass NunchakuFeedForward(FeedForward):\n    \"\"\"\n    Feed-forward network with fused quantized layers and optional fused GELU-MLP.\n\n    Parameters\n    ----------\n    dim : int\n        Input feature dimension.\n    dim_out : int, optional\n        Output feature dimension. If None, set to `dim`.\n    mult : int, optional\n        Expansion factor for the hidden layer (default: 4).\n    dropout : float, optional\n        Dropout probability (default: 0.0).\n    inner_dim : int, optional\n        Hidden layer dimension. If None, computed as `dim * mult`.\n    bias : bool, optional\n        Whether to use bias in the projections (default: True).\n    dtype : torch.dtype, optional\n        Data type for the projections.\n    device : torch.device, optional\n        Device for the projections.\n    **kwargs\n        Additional arguments for the quantized linear layers.\n    \"\"\"\n\n    def __init__(self, dim: int, dim_out: int | None=None, mult: int=4, dropout: float=0.0, inner_dim=None, bias: bool=True, dtype=None, device=None, **kwargs):\n        super(FeedForward, self).__init__()\n        if inner_dim is None:\n            inner_dim = int(dim * mult)\n        dim_out = dim_out if dim_out is not None else dim\n        self.net = nn.ModuleList([])\n        self.net.append(NunchakuGELU(dim, inner_dim, approximate='tanh', bias=bias, dtype=dtype, device=device, **kwargs))\n        self.net.append(nn.Dropout(dropout))\n        self.net.append(SVDQW4A4Linear(inner_dim, dim_out, bias=bias, act_unsigned=kwargs['precision'] == 'int4', torch_dtype=dtype, device=device, **kwargs))\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the feed-forward network.\n\n        Parameters\n        ----------\n        hidden_states : torch.Tensor\n            Input tensor of shape (batch, seq_len, dim).\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor after feed-forward transformation.\n        \"\"\"\n        if isinstance(self.net[0], NunchakuGELU):\n            return fused_gelu_mlp(hidden_states, self.net[0].proj, self.net[2])\n        else:\n            for module in self.net:\n                hidden_states = module(hidden_states)\n            return hidden_states\n\nclass VAELoader:\n\n    @staticmethod\n    def vae_list():\n        vaes = folder_paths.get_filename_list('vae')\n        approx_vaes = folder_paths.get_filename_list('vae_approx')\n        sdxl_taesd_enc = False\n        sdxl_taesd_dec = False\n        sd1_taesd_enc = False\n        sd1_taesd_dec = False\n        sd3_taesd_enc = False\n        sd3_taesd_dec = False\n        f1_taesd_enc = False\n        f1_taesd_dec = False\n        for v in approx_vaes:\n            if v.startswith('taesd_decoder.'):\n                sd1_taesd_dec = True\n            elif v.startswith('taesd_encoder.'):\n                sd1_taesd_enc = True\n            elif v.startswith('taesdxl_decoder.'):\n                sdxl_taesd_dec = True\n            elif v.startswith('taesdxl_encoder.'):\n                sdxl_taesd_enc = True\n            elif v.startswith('taesd3_decoder.'):\n                sd3_taesd_dec = True\n            elif v.startswith('taesd3_encoder.'):\n                sd3_taesd_enc = True\n            elif v.startswith('taef1_encoder.'):\n                f1_taesd_dec = True\n            elif v.startswith('taef1_decoder.'):\n                f1_taesd_enc = True\n        if sd1_taesd_dec and sd1_taesd_enc:\n            vaes.append('taesd')\n        if sdxl_taesd_dec and sdxl_taesd_enc:\n            vaes.append('taesdxl')\n        if sd3_taesd_dec and sd3_taesd_enc:\n            vaes.append('taesd3')\n        if f1_taesd_dec and f1_taesd_enc:\n            vaes.append('taef1')\n        vaes.append('pixel_space')\n        return vaes\n\n    @staticmethod\n    def load_taesd(name):\n        sd = {}\n        approx_vaes = folder_paths.get_filename_list('vae_approx')\n        encoder = next(filter(lambda a: a.startswith('{}_encoder.'.format(name)), approx_vaes))\n        decoder = next(filter(lambda a: a.startswith('{}_decoder.'.format(name)), approx_vaes))\n        enc = comfy.utils.load_torch_file(folder_paths.get_full_path_or_raise('vae_approx', encoder))\n        for k in enc:\n            sd['taesd_encoder.{}'.format(k)] = enc[k]\n        dec = comfy.utils.load_torch_file(folder_paths.get_full_path_or_raise('vae_approx', decoder))\n        for k in dec:\n            sd['taesd_decoder.{}'.format(k)] = dec[k]\n        if name == 'taesd':\n            sd['vae_scale'] = torch.tensor(0.18215)\n            sd['vae_shift'] = torch.tensor(0.0)\n        elif name == 'taesdxl':\n            sd['vae_scale'] = torch.tensor(0.13025)\n            sd['vae_shift'] = torch.tensor(0.0)\n        elif name == 'taesd3':\n            sd['vae_scale'] = torch.tensor(1.5305)\n            sd['vae_shift'] = torch.tensor(0.0609)\n        elif name == 'taef1':\n            sd['vae_scale'] = torch.tensor(0.3611)\n            sd['vae_shift'] = torch.tensor(0.1159)\n        return sd\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {'required': {'vae_name': (s.vae_list(),)}}\n    RETURN_TYPES = ('VAE',)\n    FUNCTION = 'load_vae'\n    CATEGORY = 'loaders'\n\n    def load_vae(self, vae_name):\n        if vae_name == 'pixel_space':\n            sd = {}\n            sd['pixel_space_vae'] = torch.tensor(1.0)\n        elif vae_name in ['taesd', 'taesdxl', 'taesd3', 'taef1']:\n            sd = self.load_taesd(vae_name)\n        else:\n            vae_path = folder_paths.get_full_path_or_raise('vae', vae_name)\n            sd = comfy.utils.load_torch_file(vae_path)\n        vae = comfy.sd.VAE(sd=sd)\n        vae.throw_exception_if_invalid()\n        return (vae,)\n\nclass NunchakuQwenImageTransformerBlock(nn.Module):\n    \"\"\"\n    Transformer block with dual-stream (image/text) processing, modulation, and quantized attention/MLP.\n\n    Parameters\n    ----------\n    dim : int\n        Input feature dimension.\n    num_attention_heads : int\n        Number of attention heads.\n    attention_head_dim : int\n        Dimension per attention head.\n    eps : float, optional\n        Epsilon for normalization layers (default: 1e-6).\n    dtype : torch.dtype, optional\n        Data type for projections.\n    device : torch.device, optional\n        Device for projections.\n    operations : module, optional\n        Module providing normalization and linear layers.\n    scale_shift : float, optional\n        Value added to scale in modulation (default: 1.0). Nunchaku may have fused the scale's shift into bias.\n    **kwargs\n        Additional arguments for quantized linear layers.\n    \"\"\"\n\n    def __init__(self, dim: int, num_attention_heads: int, attention_head_dim: int, eps: float=1e-06, dtype=None, device=None, operations=None, scale_shift: float=1.0, **kwargs):\n        super().__init__()\n        self.scale_shift = scale_shift\n        self.dim = dim\n        self.num_attention_heads = num_attention_heads\n        self.attention_head_dim = attention_head_dim\n        self.img_mod = nn.Sequential(nn.SiLU(), AWQW4A16Linear(dim, 6 * dim, bias=True, torch_dtype=dtype, device=device))\n        self.img_norm1 = operations.LayerNorm(dim, elementwise_affine=False, eps=eps, dtype=dtype, device=device)\n        self.img_norm2 = operations.LayerNorm(dim, elementwise_affine=False, eps=eps, dtype=dtype, device=device)\n        self.img_mlp = NunchakuFeedForward(dim=dim, dim_out=dim, dtype=dtype, device=device, **kwargs)\n        self.txt_mod = nn.Sequential(nn.SiLU(), AWQW4A16Linear(dim, 6 * dim, bias=True, torch_dtype=dtype, device=device))\n        self.txt_norm1 = operations.LayerNorm(dim, elementwise_affine=False, eps=eps, dtype=dtype, device=device)\n        self.txt_norm2 = operations.LayerNorm(dim, elementwise_affine=False, eps=eps, dtype=dtype, device=device)\n        self.txt_mlp = NunchakuFeedForward(dim=dim, dim_out=dim, dtype=dtype, device=device, **kwargs)\n        self.attn = Attention_custom_nodes_ComfyUI_nunchaku_models(query_dim=dim, dim_head=attention_head_dim, heads=num_attention_heads, out_dim=dim, bias=True, eps=eps, dtype=dtype, device=device, operations=operations, **kwargs)\n\n    def _modulate(self, x: torch.Tensor, mod_params: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Apply modulation to input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (batch, seq_len, dim).\n        mod_params : torch.Tensor\n            Modulation parameters of shape (batch, 3*dim).\n\n        Returns\n        -------\n        modulated_x : torch.Tensor\n            Modulated tensor.\n        gate : torch.Tensor\n            Gate tensor for residual connection.\n        \"\"\"\n        shift, scale, gate = mod_params.chunk(3, dim=-1)\n        if self.scale_shift != 0:\n            scale.add_(self.scale_shift)\n        return (x * scale.unsqueeze(1) + shift.unsqueeze(1), gate.unsqueeze(1))\n\n    def forward(self, hidden_states: torch.Tensor, encoder_hidden_states: torch.Tensor, encoder_hidden_states_mask: torch.Tensor, temb: torch.Tensor, image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]]=None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass for the transformer block.\n\n        Parameters\n        ----------\n        hidden_states : torch.Tensor\n            Image stream input tensor.\n        encoder_hidden_states : torch.Tensor\n            Text stream input tensor.\n        encoder_hidden_states_mask : torch.Tensor\n            Mask for encoder hidden states.\n        temb : torch.Tensor\n            Timestep or conditioning embedding.\n        image_rotary_emb : tuple of torch.Tensor, optional\n            Rotary positional embeddings.\n\n        Returns\n        -------\n        encoder_hidden_states : torch.Tensor\n            Updated text stream tensor.\n        hidden_states : torch.Tensor\n            Updated image stream tensor.\n        \"\"\"\n        img_mod_params = self.img_mod(temb)\n        txt_mod_params = self.txt_mod(temb)\n        img_mod_params = img_mod_params.view(img_mod_params.shape[0], -1, 6).transpose(1, 2).reshape(img_mod_params.shape[0], -1)\n        txt_mod_params = txt_mod_params.view(txt_mod_params.shape[0], -1, 6).transpose(1, 2).reshape(txt_mod_params.shape[0], -1)\n        img_mod1, img_mod2 = img_mod_params.chunk(2, dim=-1)\n        txt_mod1, txt_mod2 = txt_mod_params.chunk(2, dim=-1)\n        img_normed = self.img_norm1(hidden_states)\n        img_modulated, img_gate1 = self._modulate(img_normed, img_mod1)\n        txt_normed = self.txt_norm1(encoder_hidden_states)\n        txt_modulated, txt_gate1 = self._modulate(txt_normed, txt_mod1)\n        attn_output = self.attn(hidden_states=img_modulated, encoder_hidden_states=txt_modulated, encoder_hidden_states_mask=encoder_hidden_states_mask, image_rotary_emb=image_rotary_emb)\n        img_attn_output, txt_attn_output = attn_output\n        hidden_states = hidden_states + img_gate1 * img_attn_output\n        encoder_hidden_states = encoder_hidden_states + txt_gate1 * txt_attn_output\n        img_normed2 = self.img_norm2(hidden_states)\n        img_modulated2, img_gate2 = self._modulate(img_normed2, img_mod2)\n        img_mlp_output = self.img_mlp(img_modulated2)\n        hidden_states = hidden_states + img_gate2 * img_mlp_output\n        txt_normed2 = self.txt_norm2(encoder_hidden_states)\n        txt_modulated2, txt_gate2 = self._modulate(txt_normed2, txt_mod2)\n        txt_mlp_output = self.txt_mlp(txt_modulated2)\n        encoder_hidden_states = encoder_hidden_states + txt_gate2 * txt_mlp_output\n        return (encoder_hidden_states, hidden_states)\n\nclass NunchakuQwenImageTransformer2DModel(NunchakuModelMixin, QwenImageTransformer2DModel):\n    \"\"\"\n    Full transformer model for QwenImage, using Nunchaku-optimized blocks.\n\n    Parameters\n    ----------\n    patch_size : int, optional\n        Patch size for image input (default: 2).\n    in_channels : int, optional\n        Number of input channels (default: 64).\n    out_channels : int, optional\n        Number of output channels (default: 16).\n    num_layers : int, optional\n        Number of transformer layers (default: 60).\n    attention_head_dim : int, optional\n        Dimension per attention head (default: 128).\n    num_attention_heads : int, optional\n        Number of attention heads (default: 24).\n    joint_attention_dim : int, optional\n        Dimension for joint attention (default: 3584).\n    pooled_projection_dim : int, optional\n        Dimension for pooled projection (default: 768).\n    guidance_embeds : bool, optional\n        Whether to use guidance embeddings (default: False).\n    axes_dims_rope : tuple of int, optional\n        Axes dimensions for rotary embeddings (default: (16, 56, 56)).\n    image_model : module, optional\n        Optional image model.\n    dtype : torch.dtype, optional\n        Data type for projections.\n    device : torch.device, optional\n        Device for projections.\n    operations : module, optional\n        Module providing normalization and linear layers.\n    scale_shift : float, optional\n        Value added to scale in modulation (default: 1.0).\n    **kwargs\n        Additional arguments for quantized linear layers.\n    \"\"\"\n\n    def __init__(self, patch_size: int=2, in_channels: int=64, out_channels: Optional[int]=16, num_layers: int=60, attention_head_dim: int=128, num_attention_heads: int=24, joint_attention_dim: int=3584, pooled_projection_dim: int=768, guidance_embeds: bool=False, axes_dims_rope: Tuple[int, int, int]=(16, 56, 56), image_model=None, dtype=None, device=None, operations=None, scale_shift: float=1.0, **kwargs):\n        super(QwenImageTransformer2DModel, self).__init__()\n        self.dtype = dtype\n        self.patch_size = patch_size\n        self.out_channels = out_channels or in_channels\n        self.inner_dim = num_attention_heads * attention_head_dim\n        self.pe_embedder = EmbedND(dim=attention_head_dim, theta=10000, axes_dim=list(axes_dims_rope))\n        self.time_text_embed = QwenTimestepProjEmbeddings(embedding_dim=self.inner_dim, pooled_projection_dim=pooled_projection_dim, dtype=dtype, device=device, operations=operations)\n        self.txt_norm = operations.RMSNorm(joint_attention_dim, eps=1e-06, dtype=dtype, device=device)\n        self.img_in = operations.Linear(in_channels, self.inner_dim, dtype=dtype, device=device)\n        self.txt_in = operations.Linear(joint_attention_dim, self.inner_dim, dtype=dtype, device=device)\n        self.transformer_blocks = nn.ModuleList([NunchakuQwenImageTransformerBlock(dim=self.inner_dim, num_attention_heads=num_attention_heads, attention_head_dim=attention_head_dim, dtype=dtype, device=device, operations=operations, scale_shift=scale_shift, **kwargs) for _ in range(num_layers)])\n        self.norm_out = LastLayer(self.inner_dim, self.inner_dim, dtype=dtype, device=device, operations=operations)\n        self.proj_out = operations.Linear(self.inner_dim, patch_size * patch_size * self.out_channels, bias=True, dtype=dtype, device=device)\n        self.gradient_checkpointing = False\n\n    def _forward(self, x, timesteps, context, attention_mask=None, guidance: torch.Tensor=None, ref_latents=None, transformer_options={}, control=None, **kwargs):\n        \"\"\"\n        Forward pass of the Nunchaku Qwen-Image model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input image tensor of shape (batch, channels, height, width).\n        timesteps : torch.Tensor or int\n            Timestep(s) for diffusion process.\n        context : torch.Tensor\n            Textual context tensor (e.g., from a text encoder).\n        attention_mask : torch.Tensor, optional\n            Optional attention mask for the context.\n        guidance : torch.Tensor, optional\n            Optional guidance tensor for classifier-free guidance.\n        ref_latents : list[torch.Tensor], optional\n            Optional list of reference latent tensors for multi-image conditioning.\n        transformer_options : dict, optional\n            Dictionary of options for transformer block patching and replacement.\n        **kwargs\n            Additional keyword arguments. Supports 'ref_latents_method' to control reference latent handling.\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape (batch, channels, height, width), matching the input spatial dimensions.\n\n        \"\"\"\n        device = x.device\n        if self.offload:\n            self.offload_manager.set_device(device)\n        timestep = timesteps\n        encoder_hidden_states = context\n        encoder_hidden_states_mask = attention_mask\n        hidden_states, img_ids, orig_shape = self.process_img(x)\n        num_embeds = hidden_states.shape[1]\n        if ref_latents is not None:\n            h = 0\n            w = 0\n            index = 0\n            index_ref_method = kwargs.get('ref_latents_method', 'index') == 'index'\n            for ref in ref_latents:\n                if index_ref_method:\n                    index += 1\n                    h_offset = 0\n                    w_offset = 0\n                else:\n                    index = 1\n                    h_offset = 0\n                    w_offset = 0\n                    if ref.shape[-2] + h > ref.shape[-1] + w:\n                        w_offset = w\n                    else:\n                        h_offset = h\n                    h = max(h, ref.shape[-2] + h_offset)\n                    w = max(w, ref.shape[-1] + w_offset)\n                kontext, kontext_ids, _ = self.process_img(ref, index=index, h_offset=h_offset, w_offset=w_offset)\n                hidden_states = torch.cat([hidden_states, kontext], dim=1)\n                img_ids = torch.cat([img_ids, kontext_ids], dim=1)\n        txt_start = round(max((x.shape[-1] + self.patch_size // 2) // self.patch_size // 2, (x.shape[-2] + self.patch_size // 2) // self.patch_size // 2))\n        txt_ids = torch.arange(txt_start, txt_start + context.shape[1], device=x.device).reshape(1, -1, 1).repeat(x.shape[0], 1, 3)\n        ids = torch.cat((txt_ids, img_ids), dim=1)\n        image_rotary_emb = self.pe_embedder(ids).squeeze(1).unsqueeze(2).to(x.dtype)\n        del ids, txt_ids, img_ids\n        hidden_states = self.img_in(hidden_states)\n        encoder_hidden_states = self.txt_norm(encoder_hidden_states)\n        encoder_hidden_states = self.txt_in(encoder_hidden_states)\n        if guidance is not None:\n            guidance = guidance * 1000\n        temb = self.time_text_embed(timestep, hidden_states) if guidance is None else self.time_text_embed(timestep, guidance, hidden_states)\n        patches_replace = transformer_options.get('patches_replace', {})\n        blocks_replace = patches_replace.get('dit', {})\n        compute_stream = torch.cuda.current_stream()\n        if self.offload:\n            self.offload_manager.initialize(compute_stream)\n        for i, block in enumerate(self.transformer_blocks):\n            with torch.cuda.stream(compute_stream):\n                if self.offload:\n                    block = self.offload_manager.get_block(i)\n                if ('double_block', i) in blocks_replace:\n\n                    def block_wrap(args):\n                        out = {}\n                        out['txt'], out['img'] = block(hidden_states=args['img'], encoder_hidden_states=args['txt'], encoder_hidden_states_mask=encoder_hidden_states_mask, temb=args['vec'], image_rotary_emb=args['pe'])\n                        return out\n                    out = blocks_replace['double_block', i]({'img': hidden_states, 'txt': encoder_hidden_states, 'vec': temb, 'pe': image_rotary_emb}, {'original_block': block_wrap})\n                    hidden_states = out['img']\n                    encoder_hidden_states = out['txt']\n                else:\n                    encoder_hidden_states, hidden_states = block(hidden_states=hidden_states, encoder_hidden_states=encoder_hidden_states, encoder_hidden_states_mask=encoder_hidden_states_mask, temb=temb, image_rotary_emb=image_rotary_emb)\n                _control = control if control is not None else transformer_options.get('control', None) if isinstance(transformer_options, dict) else None\n                if isinstance(_control, dict):\n                    control_i = _control.get('input')\n                    try:\n                        _scale = float(_control.get('weight', _control.get('scale', 1.0)))\n                    except Exception:\n                        _scale = 1.0\n                else:\n                    control_i = None\n                    _scale = 1.0\n                if control_i is not None and i < len(control_i):\n                    add = control_i[i]\n                    if add is not None:\n                        if getattr(add, 'device', None) != hidden_states.device or getattr(add, 'dtype', None) != hidden_states.dtype:\n                            add = add.to(device=hidden_states.device, dtype=hidden_states.dtype, non_blocking=True)\n                        t = min(hidden_states.shape[1], add.shape[1])\n                        if t > 0:\n                            hidden_states[:, :t].add_(add[:, :t], alpha=_scale)\n            if self.offload:\n                self.offload_manager.step(compute_stream)\n        hidden_states = self.norm_out(hidden_states, temb)\n        hidden_states = self.proj_out(hidden_states)\n        hidden_states = hidden_states[:, :num_embeds].view(orig_shape[0], orig_shape[-2] // 2, orig_shape[-1] // 2, orig_shape[1], 2, 2)\n        hidden_states = hidden_states.permute(0, 3, 1, 4, 2, 5)\n        return hidden_states.reshape(orig_shape)[:, :, :, :x.shape[-2], :x.shape[-1]]\n\n    def set_offload(self, offload: bool, **kwargs):\n        \"\"\"\n        Enable or disable CPU offloading for the transformer blocks.\n\n        Parameters\n        ----------\n        offload : bool\n            If True, enable CPU offloading. If False, disable it.\n        **kwargs\n            Additional keyword arguments:\n                - use_pin_memory (bool): Whether to use pinned memory (default: True).\n                - num_blocks_on_gpu (int): Number of transformer blocks to keep on GPU (default: 1).\n\n        Notes\n        -----\n        - When offloading is enabled, only a subset of modules remain on GPU.\n        - When disabling, memory is released and CUDA cache is cleared.\n        \"\"\"\n        if offload == self.offload:\n            return\n        self.offload = offload\n        if offload:\n            self.offload_manager = CPUOffloadManager(self.transformer_blocks, use_pin_memory=kwargs.get('use_pin_memory', True), on_gpu_modules=[self.img_in, self.txt_in, self.txt_norm, self.time_text_embed, self.norm_out, self.proj_out], num_blocks_on_gpu=kwargs.get('num_blocks_on_gpu', 1))\n        else:\n            self.offload_manager = None\n            gc.collect()\n            torch.cuda.empty_cache()\n\nclass NunchakuQwenImage(comfy.model_base.QwenImage):\n    \"\"\"\n    Wrapper for the Nunchaku Qwen-Image model.\n\n    Parameters\n    ----------\n    model_config : object\n        Model configuration object.\n    model_type : ModelType, optional\n        Type of the model (default is ModelType.FLUX).\n    device : torch.device or str, optional\n        Device to load the model onto.\n    \"\"\"\n\n    def __init__(self, model_config, model_type=ModelType.FLUX, device=None):\n        \"\"\"\n        Initialize the NunchakuQwenImage model.\n\n        Parameters\n        ----------\n        model_config : object\n            Model configuration object.\n        model_type : ModelType, optional\n            Type of the model (default is ModelType.FLUX).\n        device : torch.device or str, optional\n            Device to load the model onto.\n        \"\"\"\n        super(comfy.model_base.QwenImage, self).__init__(model_config, model_type, device=device, unet_model=NunchakuQwenImageTransformer2DModel)\n        self.memory_usage_factor_conds = ('ref_latents',)\n\n    def load_model_weights(self, sd: dict[str, torch.Tensor], unet_prefix: str=''):\n        \"\"\"\n        Load model weights into the diffusion model.\n\n        Parameters\n        ----------\n        sd : dict of str to torch.Tensor\n            State dictionary containing model weights.\n        unet_prefix : str, optional\n            Prefix for UNet weights (default is \"\").\n\n        Raises\n        ------\n        ValueError\n            If a required key is missing from the state dictionary.\n        \"\"\"\n        diffusion_model = self.diffusion_model\n        state_dict = diffusion_model.state_dict()\n        for k in state_dict.keys():\n            if k not in sd:\n                if '.wcscales' not in k:\n                    raise ValueError(f'Key {k} not found in state_dict')\n                sd[k] = torch.ones_like(state_dict[k])\n        for n, m in diffusion_model.named_modules():\n            if isinstance(m, SVDQW4A4Linear):\n                if m.wtscale is not None:\n                    m.wtscale = sd.pop(f'{n}.wtscale', 1.0)\n        diffusion_model.load_state_dict(sd, strict=True)\n\ndef load_diffusion_model_state_dict(sd: dict[str, torch.Tensor], metadata: dict[str, str]={}, model_options: dict={}):\n    \"\"\"\n    Load a Nunchaku-quantized Qwen-Image diffusion model.\n\n    Parameters\n    ----------\n    sd : dict[str, torch.Tensor]\n        The state dictionary of the model.\n    metadata : dict[str, str], optional\n        Metadata containing quantization configuration (default is empty dict).\n    model_options : dict, optional\n        Additional model options such as dtype or custom operations.\n\n    Returns\n    -------\n    comfy.model_patcher.ModelPatcher\n        The patched and loaded Qwen-Image model ready for inference.\n    \"\"\"\n    quantization_config = json.loads(metadata.get('quantization_config', '{}'))\n    precision = get_precision_from_quantization_config(quantization_config)\n    rank = quantization_config.get('rank', 32)\n    dtype = model_options.get('dtype', None)\n    diffusion_model_prefix = model_detection.unet_prefix_from_state_dict(sd)\n    temp_sd = comfy.utils.state_dict_prefix_replace(sd, {diffusion_model_prefix: ''}, filter_keys=True)\n    if len(temp_sd) > 0:\n        sd = temp_sd\n    parameters = comfy.utils.calculate_parameters(sd)\n    weight_dtype = comfy.utils.weight_dtype(sd)\n    load_device = model_management.get_torch_device()\n    check_hardware_compatibility(quantization_config, load_device)\n    model_config = NunchakuQwenImage_custom_nodes_ComfyUI_nunchaku_model_configs({'image_model': 'qwen_image', 'scale_shift': 0, 'rank': rank, 'precision': precision})\n    model_config.optimizations['fp8'] = False\n    new_sd = sd\n    offload_device = model_management.unet_offload_device()\n    unet_weight_dtype = list(model_config.supported_inference_dtypes)\n    if model_config.scaled_fp8 is not None:\n        weight_dtype = None\n    if dtype is None:\n        unet_dtype = model_management.unet_dtype(model_params=parameters, supported_dtypes=unet_weight_dtype, weight_dtype=weight_dtype)\n    else:\n        unet_dtype = dtype\n    manual_cast_dtype = model_management.unet_manual_cast(unet_dtype, load_device, model_config.supported_inference_dtypes)\n    model_config.set_inference_dtype(unet_dtype, manual_cast_dtype)\n    model_config.custom_operations = model_options.get('custom_operations', model_config.custom_operations)\n    if model_options.get('fp8_optimizations', False):\n        model_config.optimizations['fp8'] = True\n    model = model_config.get_model(new_sd, '')\n    model = model.to(offload_device)\n    model.load_model_weights(new_sd, '')\n    return NunchakuModelPatcher(model, load_device=load_device, offload_device=offload_device)\n\nclass NunchakuQwenImage_custom_nodes_ComfyUI_nunchaku_model_configs(QwenImage):\n    \"\"\"\n    Wrapper for the Nunchaku Qwen-Image model configuration.\n    \"\"\"\n\n    def get_model(self, state_dict: dict[str, torch.Tensor], prefix: str='', device=None, **kwargs) -> NunchakuQwenImage:\n        \"\"\"\n        Instantiate and return a NunchakuQwenImage model.\n\n        Parameters\n        ----------\n        state_dict : dict\n            Model state dictionary.\n        prefix : str, optional\n            Prefix for parameter names (default is \"\").\n        device : torch.device or str, optional\n            Device to load the model onto.\n        **kwargs\n            Additional keyword arguments for model initialization.\n\n        Returns\n        -------\n        NunchakuQwenImage\n            Instantiated NunchakuQwenImage model.\n        \"\"\"\n        out = NunchakuQwenImage(self, device=device, **kwargs)\n        return out\n\nclass NunchakuQwenImageDiTLoader:\n    \"\"\"\n    Loader for Nunchaku Qwen-Image models.\n\n    Attributes\n    ----------\n    RETURN_TYPES : tuple\n        Output types for the node (\"MODEL\",).\n    FUNCTION : str\n        Name of the function to call (\"load_model\").\n    CATEGORY : str\n        Node category (\"Nunchaku\").\n    TITLE : str\n        Node title (\"Nunchaku Qwen-Image DiT Loader\").\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        \"\"\"\n        Define the input types and tooltips for the node.\n\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        \"\"\"\n        return {'required': {'model_name': (folder_paths.get_filename_list('diffusion_models'), {'tooltip': 'The Nunchaku Qwen-Image model.'}), 'cpu_offload': (['auto', 'enable', 'disable'], {'default': 'auto', 'tooltip': \"Whether to enable CPU offload for the transformer model.auto' will enable it if the GPU memory is less than 15G.\"})}, 'optional': {'num_blocks_on_gpu': ('INT', {'default': 1, 'min': 1, 'max': 60, 'tooltip': 'When CPU offload is enabled, this option determines how many transformer blocks remain on GPU memory. Increasing this value decreases CPU RAM usage but increases GPU memory usage.'}), 'use_pin_memory': (['enable', 'disable'], {'default': 'disable', 'tooltip': 'Enable this to use pinned memory for transformer blocks when CPU offload is enabled. This can improve data transfer speed between CPU and GPU, but may increase system memory usage.'})}}\n    RETURN_TYPES = ('MODEL',)\n    FUNCTION = 'load_model'\n    CATEGORY = 'Nunchaku'\n    TITLE = 'Nunchaku Qwen-Image DiT Loader'\n\n    def load_model(self, model_name: str, cpu_offload: str, num_blocks_on_gpu: int=1, use_pin_memory: str='disable', **kwargs):\n        \"\"\"\n        Load the Qwen-Image model from file and return a patched model.\n\n        Parameters\n        ----------\n        model_name : str\n            The filename of the Qwen-Image model to load.\n        cpu_offload : str\n            Whether to enable CPU offload for the transformer model.\n        num_blocks_on_gpu : int\n            The number of transformer blocks to keep on GPU when CPU offload is enabled.\n        use_pin_memory : str\n            Whether to use pinned memory for the transformer blocks when CPU offload is enabled.\n\n        Returns\n        -------\n        tuple\n            A tuple containing the loaded and patched model.\n        \"\"\"\n        model_path = folder_paths.get_full_path_or_raise('diffusion_models', model_name)\n        sd, metadata = comfy.utils.load_torch_file(model_path, return_metadata=True)\n        model = load_diffusion_model_state_dict(sd, metadata=metadata)\n        if cpu_offload == 'auto':\n            if get_gpu_memory() < 15:\n                cpu_offload_enabled = True\n                logger.info('VRAM < 15GiB, enabling CPU offload')\n            else:\n                cpu_offload_enabled = False\n                logger.info('VRAM > 15GiB, disabling CPU offload')\n        elif cpu_offload == 'enable':\n            cpu_offload_enabled = True\n            logger.info('Enabling CPU offload')\n        else:\n            assert cpu_offload == 'disable', 'Invalid CPU offload option'\n            cpu_offload_enabled = False\n            logger.info('Disabling CPU offload')\n        if cpu_offload_enabled:\n            assert use_pin_memory in ['enable', 'disable'], 'Invalid use_pin_memory option'\n            model.model.diffusion_model.set_offload(cpu_offload_enabled, num_blocks_on_gpu=num_blocks_on_gpu, use_pin_memory=use_pin_memory == 'enable')\n        return (model,)\n\n# --- Nœud principal du sous-graphe ---\nclass Qwen_edit_all_in_one:\n    @classmethod\n    def INPUT_TYPES(s):\n        return { \"required\": {\n            \"model_name\": (folder_paths.get_filename_list('diffusion_models'), {'tooltip': 'The Nunchaku Qwen-Image model.'}),\n            \"clip_name\": (folder_paths.get_filename_list('text_encoders'),),\n            \"type\": (['stable_diffusion', 'stable_cascade', 'sd3', 'stable_audio', 'mochi', 'ltxv', 'pixart', 'cosmos', 'lumina2', 'wan', 'hidream', 'chroma', 'ace', 'omnigen2', 'qwen_image', 'hunyuan_image'],),\n            \"vae_name\": (folder_paths.get_filename_list('vae'),),\n            \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, {'tooltip': 'The algorithm used when sampling, this can affect the quality, speed, and style of the generated output.'}),\n            \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, {'tooltip': 'The scheduler controls how noise is gradually removed to form the image.'}),\n            \"steps\": (\"INT\", {'default': 20, 'min': 1, 'max': 10000, 'tooltip': 'The number of steps used in the denoising process.'}),\n            \"cfg\": (\"FLOAT\", {'default': 8.0, 'min': 0.0, 'max': 100.0, 'step': 0.1, 'round': 0.01, 'tooltip': 'The Classifier-Free Guidance scale balances creativity and adherence to the prompt. Higher values result in images more closely matching the prompt however too high values will negatively impact quality.'}),\n            \"denoise\": (\"FLOAT\", {'default': 1.0, 'min': 0.0, 'max': 1.0, 'step': 0.01, 'tooltip': 'The amount of denoising applied, lower values will maintain the structure of the initial image allowing for image to image sampling.'}),\n            \"prompt\": (\"STRING\", {'multiline': True, 'dynamicPrompts': True}),\n            \"prompt_1\": (\"STRING\", {'multiline': True, 'dynamicPrompts': True}),\n            \"cpu_offload\": (['auto', 'enable', 'disable'], {'default': 'auto', 'tooltip': \"Whether to enable CPU offload for the transformer model.auto' will enable it if the GPU memory is less than 15G.\"}),\n            \"seed\": (\"INT\", {'default': 0, 'min': 0, 'max': 18446744073709551615, 'control_after_generate': True, 'tooltip': 'The random seed used for creating the noise.'}),\n            \"image1\": (\"IMAGE\",),\n            \"image2\": (\"IMAGE\",),\n            \"image3\": (\"IMAGE\",),\n            \"image1_1\": (\"IMAGE\",),\n            \"image2_1\": (\"IMAGE\",),\n            \"image3_1\": (\"IMAGE\",),\n            \"pixels\": (\"IMAGE\",),\n        }}\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"IMAGE\",)\n    FUNCTION = \"execute\"\n    CATEGORY = \"_my_nodes/custom\"\n\n    def execute(self, model_name, clip_name, type, vae_name, sampler_name, scheduler, steps, cfg, denoise, prompt, prompt_1, cpu_offload, seed, image1, image2, image3, image1_1, image2_1, image3_1, pixels):\n\n        Load_CLIP_38 = CLIPLoader()\n        (out_38_0,) = Load_CLIP_38.load_clip(clip_name=clip_name, type=type)\n\n        Load_VAE_39 = VAELoader()\n        (out_39_0,) = Load_VAE_39.load_vae(vae_name=vae_name)\n\n        Nunchaku_Qwen_Image_DiT_Loader_121 = NunchakuQwenImageDiTLoader()\n        (out_121_0,) = Nunchaku_Qwen_Image_DiT_Loader_121.load_model(model_name=model_name, cpu_offload=cpu_offload)\n\n        TextEncodeQwenImageEditPlus_110 = TextEncodeQwenImageEditPlus()\n        (out_110_0,) = TextEncodeQwenImageEditPlus_110.EXECUTE_NORMALIZED(clip=out_38_0, vae=out_39_0, prompt=prompt_1, image1=image1_1, image2=image2_1, image3=image3_1)\n\n        TextEncodeQwenImageEditPlus_111 = TextEncodeQwenImageEditPlus()\n        (out_111_0,) = TextEncodeQwenImageEditPlus_111.EXECUTE_NORMALIZED(clip=out_38_0, vae=out_39_0, prompt=prompt, image1=image1, image2=image2, image3=image3)\n\n        VAE_Encode_126 = VAEEncode()\n        (out_126_0,) = VAE_Encode_126.encode(vae=out_39_0, pixels=pixels)\n\n        KSampler_122 = KSampler()\n        (out_122_0,) = KSampler_122.sample(positive=out_111_0, negative=out_110_0, latent_image=out_126_0, model=out_121_0, sampler_name=sampler_name, scheduler=scheduler, steps=steps, cfg=cfg, denoise=denoise, seed=seed)\n\n        VAE_Decode_125 = VAEDecode()\n        (out_125_0,) = VAE_Decode_125.decode(samples=out_122_0, vae=out_39_0)\n\n        return (out_125_0,)\n\n# --- Mappings pour ComfyUI ---\nNODE_CLASS_MAPPINGS = { \"Qwen_edit_all_in_one\": Qwen_edit_all_in_one }\nNODE_DISPLAY_NAME_MAPPINGS = { \"Qwen_edit_all_in_one\": \"Qwen_edit_all_in_one\" }\n","Qwen_edit_all_in_one","_my_nodes/custom","Copié dans le presse-papiers !",null,null]}],"links":[[272,126,0,131,1,"*"],[298,134,0,143,0,"IMAGE"],[299,134,0,143,6,"IMAGE"],[306,134,0,143,3,"IMAGE"],[307,143,0,117,0,"IMAGE"],[308,134,0,143,1,"IMAGE"],[309,134,0,143,4,"IMAGE"],[310,134,0,143,2,"IMAGE"],[311,134,0,143,5,"IMAGE"]],"groups":[],"definitions":{"subgraphs":[{"id":"7b4eca40-2d8f-4932-a22f-98d6507b84b6","version":1,"state":{"lastGroupId":4,"lastNodeId":126,"lastLinkId":314,"lastRerouteId":0},"revision":0,"config":{},"name":"New Subgraph","inputNode":{"id":-10,"bounding":[-430,289.81483459472656,120,440]},"outputNode":{"id":-20,"bounding":[1610,260,120,60]},"inputs":[{"id":"b5f92849-a485-45a7-92eb-fad59dcefa45","name":"model_name","type":"COMBO","linkIds":[265],"pos":[-330,309.81483459472656]},{"id":"41876509-cb8b-47f0-910c-db767d34e4e4","name":"clip_name","type":"COMBO","linkIds":[266],"pos":[-330,329.81483459472656]},{"id":"1d686d2b-1a62-4857-8580-24f0a5108166","name":"type","type":"COMBO","linkIds":[267],"pos":[-330,349.81483459472656]},{"id":"0fad1eb4-3cf8-4eee-ba47-3b55091ac1e3","name":"vae_name","type":"COMBO","linkIds":[268],"pos":[-330,369.81483459472656]},{"id":"4511a7ae-fbf1-4a7e-a4a9-3f0c1dbfd63e","name":"sampler_name","type":"COMBO","linkIds":[269],"pos":[-330,389.81483459472656]},{"id":"94152cb3-d511-48ba-8560-c8bde390fcd1","name":"scheduler","type":"COMBO","linkIds":[270],"pos":[-330,409.81483459472656]},{"id":"3cc4e622-1ff2-4275-9323-6c46a4a75ee6","name":"steps","type":"INT","linkIds":[271],"pos":[-330,429.81483459472656]},{"id":"09cce287-09f3-4153-a6ea-745a69775d0f","name":"cfg","type":"FLOAT","linkIds":[272],"pos":[-330,449.81483459472656]},{"id":"6819a3a3-d132-4d8c-9b32-16e643fa8fb8","name":"denoise","type":"FLOAT","linkIds":[273],"pos":[-330,469.81483459472656]},{"id":"ef5f4603-6d13-4a9b-9029-7f12b9417940","name":"prompt","type":"STRING","linkIds":[281],"pos":[-330,489.81483459472656]},{"id":"12151a24-0100-4e42-8e07-fe582f756e18","name":"prompt_1","type":"STRING","linkIds":[283],"pos":[-330,509.81483459472656]},{"id":"35a9c788-b0e1-4d3b-a3ef-12bf206ab1eb","name":"cpu_offload","type":"COMBO","linkIds":[296],"pos":[-330,529.8148345947266]},{"id":"d317f3e8-2d1f-41eb-b2c8-8b02605f8d29","name":"seed","type":"INT","linkIds":[297],"pos":[-330,549.8148345947266]},{"id":"1d2359a4-56a1-436b-8dcc-c885fbe49b00","name":"image1","type":"IMAGE","linkIds":[304,307,310],"pos":[-330,569.8148345947266]},{"id":"69a90d87-4ee2-4215-88ce-fb5ebbdb8591","name":"image2","type":"IMAGE","linkIds":[305,308],"pos":[-330,589.8148345947266]},{"id":"2c4e701c-79a5-4043-b3c2-c41402a8c85a","name":"image3","type":"IMAGE","linkIds":[306,309],"pos":[-330,609.8148345947266]},{"id":"34a2aa49-d0f2-4b83-8f8a-b3ef03bb27af","name":"image1_1","type":"IMAGE","linkIds":[311],"pos":[-330,629.8148345947266]},{"id":"72630198-c943-43f2-bdc8-238727284d40","name":"image2_1","type":"IMAGE","linkIds":[312],"pos":[-330,649.8148345947266]},{"id":"05a1b621-031a-462b-a30d-07877b07653c","name":"image3_1","type":"IMAGE","linkIds":[313],"pos":[-330,669.8148345947266]},{"id":"dc69a116-ccab-439d-a71d-1e5b70a1583d","name":"pixels","type":"IMAGE","linkIds":[314],"pos":[-330,689.8148345947266]}],"outputs":[{"id":"068532f8-c713-466d-87dd-5c63a099ee2e","name":"IMAGE","type":"IMAGE","linkIds":[264],"localized_name":"IMAGE","pos":[1630,280]}],"widgets":[],"nodes":[{"id":125,"type":"VAEDecode","pos":[1080,-190],"size":[210,46],"flags":{"collapsed":false},"order":6,"mode":4,"inputs":[{"localized_name":"samples","name":"samples","type":"LATENT","link":253},{"localized_name":"vae","name":"vae","type":"VAE","link":258}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[264]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.48","Node name for S&R":"VAEDecode","ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}},"enableTabs":false,"tabWidth":65,"tabXOffset":10,"hasSecondTab":false,"secondTabText":"Send Back","secondTabOffset":80,"secondTabWidth":65},"widgets_values":[]},{"id":38,"type":"CLIPLoader","pos":[-270,-110],"size":[330,110],"flags":{},"order":0,"mode":4,"inputs":[{"localized_name":"clip_name","name":"clip_name","type":"COMBO","widget":{"name":"clip_name"},"link":266},{"localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"},"link":267},{"localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"},"link":null}],"outputs":[{"localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":0,"links":[204,205]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.48","Node name for S&R":"CLIPLoader","ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}},"models":[{"name":"qwen_2.5_vl_7b_fp8_scaled.safetensors","url":"https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors","directory":"text_encoders"}],"enableTabs":false,"tabWidth":65,"tabXOffset":10,"hasSecondTab":false,"secondTabText":"Send Back","secondTabOffset":80,"secondTabWidth":65},"widgets_values":["svdq-int4_r32-qwen-image-edit-2509-lightningv2.0-4steps.safetensors","qwen_2.5_vl_7b_fp8_scaled.safetensors","default"]},{"id":39,"type":"VAELoader","pos":[-290,50],"size":[330,60],"flags":{},"order":1,"mode":4,"inputs":[{"localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"},"link":268}],"outputs":[{"localized_name":"VAE","name":"VAE","type":"VAE","slot_index":0,"links":[206,207,258,292]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.48","Node name for S&R":"VAELoader","ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}},"models":[{"name":"qwen_image_vae.safetensors","url":"https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/vae/qwen_image_vae.safetensors","directory":"vae"}],"enableTabs":false,"tabWidth":65,"tabXOffset":10,"hasSecondTab":false,"secondTabText":"Send Back","secondTabOffset":80,"secondTabWidth":65},"widgets_values":["qwen_image"]},{"id":111,"type":"TextEncodeQwenImageEditPlus","pos":[220,-90],"size":[400,200],"flags":{},"order":3,"mode":4,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":205},{"localized_name":"vae","name":"vae","shape":7,"type":"VAE","link":207},{"localized_name":"image1","name":"image1","shape":7,"type":"IMAGE","link":304},{"localized_name":"image2","name":"image2","shape":7,"type":"IMAGE","link":305},{"localized_name":"image3","name":"image3","shape":7,"type":"IMAGE","link":306},{"localized_name":"prompt","name":"prompt","type":"STRING","widget":{"name":"prompt"},"link":281}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[255]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.59","Node name for S&R":"TextEncodeQwenImageEditPlus","ue_properties":{"widget_ue_connectable":{},"version":"7.1","input_ue_unconnectable":{}}},"widgets_values":["1"],"color":"#232","bgcolor":"#353"},{"id":122,"type":"KSampler","pos":[880,120],"size":[300,474],"flags":{},"order":5,"mode":4,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":295},{"localized_name":"positive","name":"positive","type":"CONDITIONING","link":255},{"localized_name":"negative","name":"negative","type":"CONDITIONING","link":256},{"localized_name":"latent_image","name":"latent_image","type":"LATENT","link":294},{"localized_name":"seed","name":"seed","type":"INT","widget":{"name":"seed"},"link":297},{"localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":271},{"localized_name":"cfg","name":"cfg","type":"FLOAT","widget":{"name":"cfg"},"link":272},{"localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":269},{"localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":270},{"localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":273}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[253]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.48","Node name for S&R":"KSampler","ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}},"enableTabs":false,"tabWidth":65,"tabXOffset":10,"hasSecondTab":false,"secondTabText":"Send Back","secondTabOffset":80,"secondTabWidth":65},"widgets_values":["","randomize","beta",4,"qwen_image_vae.safetensors","euler",1]},{"id":121,"type":"NunchakuQwenImageDiTLoader","pos":[-260,-310],"size":[320.1197204589844,130],"flags":{},"order":4,"mode":4,"inputs":[{"localized_name":"model_name","name":"model_name","type":"COMBO","widget":{"name":"model_name"},"link":265},{"localized_name":"cpu_offload","name":"cpu_offload","type":"COMBO","widget":{"name":"cpu_offload"},"link":296},{"localized_name":"num_blocks_on_gpu","name":"num_blocks_on_gpu","shape":7,"type":"INT","widget":{"name":"num_blocks_on_gpu"},"link":null},{"localized_name":"use_pin_memory","name":"use_pin_memory","shape":7,"type":"COMBO","widget":{"name":"use_pin_memory"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[295]}],"properties":{"cnr_id":"ComfyUI-nunchaku","ver":"d9405a6305efc4dc758bdeec500fc5d695215b2b","Node name for S&R":"NunchakuQwenImageDiTLoader","ue_properties":{"widget_ue_connectable":{},"version":"7.1","input_ue_unconnectable":{}}},"widgets_values":["svdq-int4_r32-qwen-image-edit-2509-lightningv2.0-4steps.safetensors","auto",20,"disable"]},{"id":110,"type":"TextEncodeQwenImageEditPlus","pos":[220,190],"size":[400,200],"flags":{},"order":2,"mode":4,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":204},{"localized_name":"vae","name":"vae","shape":7,"type":"VAE","link":206},{"localized_name":"image1","name":"image1","shape":7,"type":"IMAGE","link":311},{"localized_name":"image2","name":"image2","shape":7,"type":"IMAGE","link":312},{"localized_name":"image3","name":"image3","shape":7,"type":"IMAGE","link":313},{"localized_name":"prompt","name":"prompt","type":"STRING","widget":{"name":"prompt"},"link":283}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[256]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.59","Node name for S&R":"TextEncodeQwenImageEditPlus","ue_properties":{"widget_ue_connectable":{},"version":"7.1","input_ue_unconnectable":{}}},"widgets_values":["the woman in picture flying"],"color":"#223","bgcolor":"#335"},{"id":126,"type":"VAEEncode","pos":[220,570],"size":[140,46],"flags":{},"order":7,"mode":4,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":314},{"localized_name":"vae","name":"vae","type":"VAE","link":292}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[294]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.64","Node name for S&R":"VAEEncode","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[]}],"groups":[],"links":[{"id":204,"origin_id":38,"origin_slot":0,"target_id":110,"target_slot":0,"type":"CLIP"},{"id":206,"origin_id":39,"origin_slot":0,"target_id":110,"target_slot":1,"type":"VAE"},{"id":205,"origin_id":38,"origin_slot":0,"target_id":111,"target_slot":0,"type":"CLIP"},{"id":207,"origin_id":39,"origin_slot":0,"target_id":111,"target_slot":1,"type":"VAE"},{"id":255,"origin_id":111,"origin_slot":0,"target_id":122,"target_slot":1,"type":"CONDITIONING"},{"id":256,"origin_id":110,"origin_slot":0,"target_id":122,"target_slot":2,"type":"CONDITIONING"},{"id":253,"origin_id":122,"origin_slot":0,"target_id":125,"target_slot":0,"type":"LATENT"},{"id":258,"origin_id":39,"origin_slot":0,"target_id":125,"target_slot":1,"type":"VAE"},{"id":264,"origin_id":125,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"IMAGE"},{"id":265,"origin_id":-10,"origin_slot":0,"target_id":121,"target_slot":0,"type":"COMBO"},{"id":266,"origin_id":-10,"origin_slot":1,"target_id":38,"target_slot":0,"type":"COMBO"},{"id":267,"origin_id":-10,"origin_slot":2,"target_id":38,"target_slot":1,"type":"COMBO"},{"id":268,"origin_id":-10,"origin_slot":3,"target_id":39,"target_slot":0,"type":"COMBO"},{"id":269,"origin_id":-10,"origin_slot":4,"target_id":122,"target_slot":7,"type":"COMBO"},{"id":270,"origin_id":-10,"origin_slot":5,"target_id":122,"target_slot":8,"type":"COMBO"},{"id":271,"origin_id":-10,"origin_slot":6,"target_id":122,"target_slot":5,"type":"INT"},{"id":272,"origin_id":-10,"origin_slot":7,"target_id":122,"target_slot":6,"type":"FLOAT"},{"id":273,"origin_id":-10,"origin_slot":8,"target_id":122,"target_slot":9,"type":"FLOAT"},{"id":281,"origin_id":-10,"origin_slot":9,"target_id":111,"target_slot":5,"type":"STRING"},{"id":283,"origin_id":-10,"origin_slot":10,"target_id":110,"target_slot":5,"type":"STRING"},{"id":292,"origin_id":39,"origin_slot":0,"target_id":126,"target_slot":1,"type":"VAE"},{"id":294,"origin_id":126,"origin_slot":0,"target_id":122,"target_slot":3,"type":"LATENT"},{"id":295,"origin_id":121,"origin_slot":0,"target_id":122,"target_slot":0,"type":"MODEL"},{"id":296,"origin_id":-10,"origin_slot":11,"target_id":121,"target_slot":1,"type":"COMBO"},{"id":297,"origin_id":-10,"origin_slot":12,"target_id":122,"target_slot":4,"type":"INT"},{"id":304,"origin_id":-10,"origin_slot":13,"target_id":111,"target_slot":2,"type":"IMAGE"},{"id":305,"origin_id":-10,"origin_slot":14,"target_id":111,"target_slot":3,"type":"IMAGE"},{"id":306,"origin_id":-10,"origin_slot":15,"target_id":111,"target_slot":4,"type":"IMAGE"},{"id":311,"origin_id":-10,"origin_slot":16,"target_id":110,"target_slot":2,"type":"IMAGE"},{"id":312,"origin_id":-10,"origin_slot":17,"target_id":110,"target_slot":3,"type":"IMAGE"},{"id":313,"origin_id":-10,"origin_slot":18,"target_id":110,"target_slot":4,"type":"IMAGE"},{"id":314,"origin_id":-10,"origin_slot":19,"target_id":126,"target_slot":0,"type":"IMAGE"}],"extra":{"ue_links":[],"links_added_by_ue":[]}}]},"config":{},"extra":{"frontendVersion":"1.26.13","ue_links":[],"links_added_by_ue":[],"VHS_latentpreview":false,"VHS_latentpreviewrate":0,"VHS_MetadataImage":true,"VHS_KeepIntermediate":true,"ds":{"scale":0.5300787401574804,"offset":[961.4200831847888,401.66666666666663]}},"version":0.4}